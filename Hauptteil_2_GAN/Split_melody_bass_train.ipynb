{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense,Concatenate, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pypianoroll\n",
    "import numpy as np\n",
    "from pypianoroll import Multitrack, Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run params\n",
    "run_id = '3001'\n",
    "music_name = 'gan/'\n",
    "\n",
    "RUN_FOLDER = 'run/'\n",
    "RUN_FOLDER += '_'.join([run_id, music_name])\n",
    "IMAGE_FOLDER = os.path.join(RUN_FOLDER, \"test_split/\")\n",
    "BASS_IMAGE_FOLDER = os.path.join(IMAGE_FOLDER, \"bass/\")\n",
    "MELODY_IMAGE_FOLDER = os.path.join(IMAGE_FOLDER, \"melody/\")\n",
    "# Number of timestept the slices Pianorolls should have (Needs to be dividable by 16)\n",
    "pianrollLength = 128\n",
    "\n",
    "store_folder = os.path.join(RUN_FOLDER, 'store')\n",
    "data_folder = os.path.join('data', music_name)\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.makedirs(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'store'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'output'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "\n",
    "if not os.path.exists(IMAGE_FOLDER):\n",
    "    os.mkdir(IMAGE_FOLDER)\n",
    "    os.mkdir(BASS_IMAGE_FOLDER)\n",
    "    os.mkdir(MELODY_IMAGE_FOLDER)\n",
    "\n",
    "#weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "weight_init = tf.keras.initializers.GlorotUniform(seed=None)\n",
    "#weight_init = tf.keras.initializers.he_uniform(seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Note bounds for faster training\n",
    "lowestNotePossible = 20\n",
    "highestNotePossible = 108\n",
    "# possibleNotes mus be dividable by 4 else the Architekture needs to be changed\n",
    "possibleNotes = highestNotePossible - lowestNotePossible\n",
    "# Bass Line\n",
    "bassNotes = 40\n",
    "melodyNotes = possibleNotes - bassNotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the numpyArray for further us\n",
    "'''\n",
    "reshaped = np.load('data/preprocessed/midi_p128_dn88.npy')\n",
    "isNormalized = False\n",
    "\n",
    "reshaped = np.load('data/preprocessed/midi_normalized_p128_dn88.npy')\n",
    "isNormalized = True\n",
    "'''\n",
    "\n",
    "reshaped = np.load('data/preprocessed/midi_binarized_p128_dn88.npy')\n",
    "isNormalized = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bass = reshaped[:,:,0:bassNotes]\n",
    "melody = reshaped[:,:,bassNotes:possibleNotes]\n",
    "\n",
    "print(melody.shape)\n",
    "print(bass.shape)\n",
    "\n",
    "reshaped = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped = bass.reshape(-1, pianrollLength, bassNotes)\n",
    "result = np.zeros((pianrollLength, 128))\n",
    "result[:,lowestNotePossible:highestNotePossible-melodyNotes] = reshaped[25]\n",
    "result.shape\n",
    "# Test convert an image back to see the Splitted Arrays Plotted\n",
    "track = Track(pianoroll=result, program=0, is_drum=False,name='my awesome piano')\n",
    "track.plot()\n",
    "#plt.savefig(os.path.join(RUN_FOLDER, \"example_pianoroll.png\"), format='png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped = melody.reshape(-1, pianrollLength, melodyNotes)\n",
    "result = np.zeros((pianrollLength, 128))\n",
    "result[:,lowestNotePossible+bassNotes:highestNotePossible] = reshaped[35]\n",
    "result.shape\n",
    "# Test convert an image back to see the Splitted Arrays Plotted\n",
    "track = Track(pianoroll=result, program=0, is_drum=False,name='my awesome piano')\n",
    "track.plot()\n",
    "#plt.savefig(os.path.join(RUN_FOLDER, \"example_pianoroll.png\"), format='png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melody Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THE Melody discriminator\n",
    "discriminator_melody_input = Input(shape=(pianrollLength,melodyNotes,1), name='discriminator_melody_input')\n",
    "\n",
    "x = discriminator_melody_input\n",
    "\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = (5,5), strides=2, kernel_initializer = weight_init, name = 'discriminator_melody_conv_0')(x)\n",
    "x = LeakyReLU()(x)\n",
    "#x = Activation('relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = (5,5), strides=2, kernel_initializer = weight_init, name = 'discriminator_melody_conv_1')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv2D(filters = 16, kernel_size = (1,5), strides=2, kernel_initializer = weight_init, name = 'discriminator_melody_conv_middle')(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "discriminator_melody_output = Dense(1, activation='sigmoid', kernel_initializer = weight_init)(x)\n",
    "discriminator_melody = Model(discriminator_melody_input, discriminator_melody_output, name= 'discriminator_melody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_melody.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 50\n",
    "\n",
    "generator_melody_input = Input(shape=(z_dim,), name='generator_melody_input')\n",
    "generator_melody_initial_dense_layer_size = (int(pianrollLength/4),int(melodyNotes/4),4)\n",
    "\n",
    "x = generator_melody_input\n",
    "x = Dense(np.prod(generator_melody_initial_dense_layer_size), kernel_initializer = weight_init)(x)\n",
    "#x = BatchNormalization(momentum=0.9)(x)\n",
    "\n",
    "#x = LeakyReLU()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Reshape(generator_melody_initial_dense_layer_size)(x)\n",
    "\n",
    "#x = UpSampling2D(size=(2, 2), data_format=None, interpolation='nearest')(x)\n",
    "#x = Conv2D(filters = 16, kernel_size = (5,5), padding='same', kernel_initializer = weight_init, name = 'generator_melody_conv_0')(x)\n",
    "x = Conv2DTranspose(\n",
    "                    filters = 16\n",
    "                    , kernel_size = (5,5)\n",
    "                    , padding = 'same'\n",
    "                    , strides = 2\n",
    "                    , name = 'generator_tconv_0'\n",
    "                    , kernel_initializer = weight_init\n",
    "                    )(x)\n",
    "#x = BatchNormalization(momentum=0.9)(x)\n",
    "#x = LeakyReLU()(x)\n",
    "x = Activation('relu')(x)  \n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "#x = UpSampling2D(size=(2, 2), data_format=None, interpolation='nearest')(x)\n",
    "#x = Conv2D(filters = 16, kernel_size = (5,5), padding='same', kernel_initializer = weight_init, name = 'generator_melody_conv_1')(x)\n",
    "x = Conv2DTranspose(\n",
    "                    filters = 32\n",
    "                    , kernel_size = (7,7)\n",
    "                    , padding = 'same'\n",
    "                    , strides = 2\n",
    "                    , name = 'generator_tconv_1'\n",
    "                    , kernel_initializer = weight_init\n",
    "                    )(x)\n",
    "#x = BatchNormalization(momentum=0.9)(x)\n",
    "#x = LeakyReLU()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv2D(filters = 16, kernel_size = (5,5), padding = 'same', kernel_initializer = weight_init, name = 'generator_melody_conv_2')(x)\n",
    "#x = BatchNormalization(momentum=0.9)(x)\n",
    "#x = LeakyReLU()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Conv2D(filters = 1, kernel_size = (5,5), padding = 'same', kernel_initializer = weight_init)(x)        \n",
    "x = Activation('sigmoid')(x)\n",
    "\n",
    "\n",
    "generator_melody_output = x\n",
    "generator_melody = Model(generator_melody_input, generator_melody_output, name='generator_melody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_melody.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meldy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THE Melody discriminator\n",
    "discriminator_bass_input = Input(shape=(pianrollLength,bassNotes,1), name='discriminator_bass_input')\n",
    "\n",
    "x = discriminator_bass_input\n",
    "\n",
    "\n",
    "x = Conv2D(filters = 16, kernel_size = (6,6), strides=2, kernel_initializer = weight_init, name = 'discriminator_bass_conv_0')(x)\n",
    "x = LeakyReLU()(x)\n",
    "#x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Conv2D(filters = 16, kernel_size = (1,6), strides=2, kernel_initializer = weight_init, name = 'discriminator_bass_conv_middle')(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "discriminator_bass_output = Dense(1, activation='sigmoid', kernel_initializer = weight_init)(x)\n",
    "discriminator_bass = Model(discriminator_bass_input, discriminator_bass_output, name= 'discriminator_bass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_bass.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 50\n",
    "\n",
    "generator_bass_input = Input(shape=(z_dim,), name='generator_bass_input')\n",
    "generator_bass_initial_dense_layer_size = (int(pianrollLength/4),int(bassNotes/4),4)\n",
    "\n",
    "x = generator_bass_input\n",
    "x = Dense(np.prod(generator_bass_initial_dense_layer_size), kernel_initializer = weight_init)(x)\n",
    "#x = BatchNormalization(momentum=0.9)(x)\n",
    "\n",
    "#x = LeakyReLU()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Reshape(generator_bass_initial_dense_layer_size)(x)\n",
    "\n",
    "x = UpSampling2D(size=(2, 2), data_format=None, interpolation='nearest')(x)\n",
    "x = Conv2D(filters = 32, kernel_size = (5,5), padding='same', kernel_initializer = weight_init, name = 'generator_bass_conv_0')(x)\n",
    "#x = BatchNormalization(momentum=0.9)(x)\n",
    "#x = LeakyReLU()(x)\n",
    "x = Activation('relu')(x)  \n",
    "\n",
    "x = UpSampling2D(size=(2, 2), data_format=None, interpolation='nearest')(x)\n",
    "x = Conv2D(filters = 32, kernel_size = (5,5), padding='same', kernel_initializer = weight_init, name = 'generator_bass_conv_1')(x)\n",
    "#x = Conv2DTranspose(\n",
    "#                    filters = (5,5)\n",
    "#                    , kernel_size = 2\n",
    "#                    , padding = 'same'\n",
    "#                    , strides = 2\n",
    "#                    , name = 'generator_tconv_0'\n",
    "#                    , kernel_initializer = weight_init\n",
    "#                    )(x)\n",
    "#x = BatchNormalization(momentum=0.9)(x)\n",
    "#x = LeakyReLU()(x)\n",
    "x = Activation('relu')(x) \n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = (5,5), padding = 'same', kernel_initializer = weight_init, name = 'generator_bass_conv_2')(x)\n",
    "#x = BatchNormalization(momentum=0.9)(x)\n",
    "#x = LeakyReLU()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Conv2D(filters = 1, kernel_size = (5,5), padding = 'same', kernel_initializer = weight_init)(x)        \n",
    "x = Activation('sigmoid')(x)\n",
    "\n",
    "\n",
    "generator_bass_output = x\n",
    "generator_bass = Model(generator_bass_input, generator_bass_output, name='generator_bass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_bass.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelle: https://stackoverflow.com/a/45199301\n",
    "# Save the summary to a file \n",
    "#from contextlib import redirect_stdout\n",
    "\n",
    "#with open(os.path.join(store_folder, 'modelsummarydiscriminator.txt'), 'w') as f:\n",
    "#    with redirect_stdout(f):\n",
    "#        discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelle: https://stackoverflow.com/a/45199301\n",
    "# Save the summary to a file \n",
    "#from contextlib import redirect_stdout\n",
    "\n",
    "#with open(os.path.join(store_folder, 'modelsummarygenerator.txt'), 'w') as f:\n",
    "#    with redirect_stdout(f):\n",
    "#        generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trainable(model, isTrainable):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = isTrainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Melody GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPILE DISCRIMINATOR\n",
    "discriminator_melody.compile(\n",
    "optimizer=RMSprop(lr=0.0004)\n",
    ", loss = 'binary_crossentropy'\n",
    ",  metrics = ['accuracy']\n",
    ")\n",
    "        \n",
    "### COMPILE THE FULL GAN\n",
    "set_trainable(discriminator_melody, False)\n",
    "\n",
    "model_input = Input(shape=(z_dim,), name='model_melody_input')\n",
    "model_output = discriminator_melody(generator_melody(model_input))\n",
    "GANModel_melody = Model(model_input, model_output)\n",
    "\n",
    "opti = RMSprop(learning_rate=0.0001)\n",
    "GANModel_melody.compile(optimizer=opti , loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "set_trainable(discriminator_melody, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(model_melody, to_file=os.path.join(RUN_FOLDER ,'viz/model_melody.png'), show_shapes = True, show_layer_names = True)\n",
    "#plot_model(discriminator_melody, to_file=os.path.join(RUN_FOLDER ,'viz/discriminator_melody.png'), show_shapes = True, show_layer_names = True)\n",
    "#plot_model(generator_melody, to_file=os.path.join(RUN_FOLDER ,'viz/generator_melody.png'), show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Bass GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### COMPILE DISCRIMINATOR\n",
    "discriminator_bass.compile(\n",
    "optimizer=RMSprop(lr=0.0004)\n",
    ", loss = 'binary_crossentropy'\n",
    ",  metrics = ['accuracy']\n",
    ")\n",
    "        \n",
    "### COMPILE THE FULL GAN\n",
    "set_trainable(discriminator_bass, False)\n",
    "\n",
    "model_input = Input(shape=(z_dim,), name='model_bass_input')\n",
    "model_output = discriminator_bass(generator_bass(model_input))\n",
    "GANModel_bass = Model(model_input, model_output)\n",
    "\n",
    "opti = RMSprop(learning_rate=0.0001)\n",
    "GANModel_bass.compile(optimizer=opti , loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "set_trainable(discriminator_bass, True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(model, to_file=os.path.join(RUN_FOLDER ,'viz/model.png'), show_shapes = True, show_layer_names = True)\n",
    "#plot_model(discriminator, to_file=os.path.join(RUN_FOLDER ,'viz/discriminator.png'), show_shapes = True, show_layer_names = True)\n",
    "#plot_model(generator, to_file=os.path.join(RUN_FOLDER ,'viz/generator.png'), show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Midi_File(img, epoch, isNormalized, isMelody):\n",
    "    if isMelody:\n",
    "        gen_img = img.reshape(128, melodyNotes)\n",
    "    else:\n",
    "        gen_img = img.reshape(128, bassNotes)\n",
    "    result = np.zeros((pianrollLength, 128))\n",
    "    if isMelody:\n",
    "        result[:,lowestNotePossible+bassNotes:highestNotePossible] = gen_img\n",
    "    else:\n",
    "        result[:,lowestNotePossible:highestNotePossible-melodyNotes] = gen_img    \n",
    "    #if isNormalized:\n",
    "    #result = (result > 0.1) * 255.\n",
    "    result = result * 255.\n",
    "    \n",
    "    track = Track(pianoroll=result, program=0, is_drum=False,name='my awesome piano')\n",
    "\n",
    "    multi = Multitrack()\n",
    "    multi.tracks[0] = track\n",
    "    pypianoroll.write(multi, os.path.join(IMAGE_FOLDER, str(epoch)+\".mid\"))\n",
    "    \n",
    "    multi.tracks[0].plot()\n",
    "    #fig.set_size_inches(100,100)\n",
    "    #plt.show()\n",
    "    plt.savefig(os.path.join(IMAGE_FOLDER, str(epoch)+\"_pianoroll.png\"), format='png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, z_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        #Rescale images 0 - 1\n",
    "\n",
    "        fig, axs = plt.subplots(r, c, figsize=(15,15))\n",
    "        cnt = 0\n",
    "\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]), cmap = 'gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(os.path.join(IMAGE_FOLDER, \"%d_multi.png\" % epoch))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_normal(epochs, batch_size=32, isMelody=True):\n",
    "    # Save the Generator and discriminator models\n",
    "    #save_model(generator, os.path.join(RUN_FOLDER, 'images/generator'))\n",
    "    #save_model(discriminator, os.path.join(RUN_FOLDER, 'images/discriminator'))\n",
    "    #save_model(GANModel, os.path.join(RUN_FOLDER, r\"images/GANModel\"))\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    # Label Smoothening\n",
    "    valid_smoothened = np.random.uniform(low=0.8, high=1.0, size=(batch_size,1))\n",
    "    fake_smoothened = np.random.uniform(low=0.0, high=0.05, size=(batch_size,1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------      \n",
    "        noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "            \n",
    "        # Select a random half of images\n",
    "        idx = np.random.randint(0, reshaped.shape[0], batch_size)\n",
    "        imgs = reshaped[idx]\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train the discriminator (real classified as ones and generated as zeros)\n",
    "        # ---------------------\n",
    "        d_loss_real, d_acc_real =  discriminator.train_on_batch(imgs, valid)\n",
    "        d_loss_fake, d_acc_fake =  discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss =  0.5 * (d_loss_real + d_loss_fake)\n",
    "        d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        g_loss = GANModel.train_on_batch(noise, valid)\n",
    "        #print (\"%d [G loss: %.3f] [G acc: %.3f]\" % (epoch, g_loss[0], g_loss[1]))\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Save Losses for evaluation\n",
    "        # ---------------------\n",
    "        d = [d_loss, d_loss_real, d_loss_fake, d_acc, d_acc_real, d_acc_fake]\n",
    "        d_losses.append(d)\n",
    "        g = [g_loss[0], g_loss[1]]\n",
    "        g_losses.append(g)\n",
    "        \n",
    "        \n",
    "        if (epoch % 100 == 0): \n",
    "            # Save an example\n",
    "            fig=plt.figure(figsize=(64, 64))\n",
    "            plt.imshow(gen_imgs[0, :, :, 0], cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.savefig(os.path.join(IMAGE_FOLDER, str(epoch)+\".png\"), format='png')\n",
    "            generate_Midi_File(gen_imgs[0, :, :, 0], epoch, isNormalized, isMelody)\n",
    "            plt.close()\n",
    "            \n",
    "            # Save some examples\n",
    "            sample_images(epoch)\n",
    "            \n",
    "            # Continuiously save a plot with the new values to see the development of the loss\n",
    "            fig = plt.figure()\n",
    "            plt.plot([x[0] for x in d_losses], color='black', linewidth=0.25)\n",
    "            plt.plot([x[1] for x in d_losses], color='green', linewidth=0.25)\n",
    "            plt.plot([x[2] for x in d_losses], color='red', linewidth=0.25)\n",
    "            plt.plot([x[0] for x in g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "            plt.xlabel('batch', fontsize=18)\n",
    "            plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "            # plt.xlim(0, 2000)\n",
    "            #plt.ylim(0, 50)\n",
    "\n",
    "            plt.savefig(os.path.join(IMAGE_FOLDER, \"loss_chart.png\"), format='png')\n",
    "            plt.show()\n",
    "            plt.close\n",
    "            # Save the loss arrays\n",
    "            np.save(os.path.join(IMAGE_FOLDER, \"D_loss.npy\"), d_losses)\n",
    "            np.save(os.path.join(IMAGE_FOLDER, \"G_loss.npy\"), g_losses)\n",
    "            \n",
    "            if (epoch % 1000 == 0):\n",
    "                GANModel.save(os.path.join(IMAGE_FOLDER, 'GANModel_'+str(epoch)+'_loss_'+str(g_loss[0])+'.h5'))\n",
    "                discriminator.save(os.path.join(IMAGE_FOLDER, 'discriminator_'+str(epoch)+'_loss_'+str(d_loss)+'.h5'))\n",
    "                generator.save(os.path.join(IMAGE_FOLDER, 'generator_'+str(epoch)+'_loss_'+str(g_loss[0])+'.h5'))\n",
    "                \n",
    "                \n",
    "        # Plot the progress\n",
    "        if (epoch % 10 == 0):\n",
    "            print (\"%d [D loss: (%.3f)(R %.3f, F %.3f)] [D acc: (%.3f)(%.3f, %.3f)] [G loss: %.3f] [G acc: %.3f]\" % (epoch, d_loss, d_loss_real, d_loss_fake, d_acc, d_acc_real, d_acc_fake, g_loss[0], g_loss[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "# Variable Initialisations\n",
    "generator = generator_melody\n",
    "discriminator = discriminator_melody\n",
    "GANModel = GANModel_melody\n",
    "IMAGE_FOLDER = MELODY_IMAGE_FOLDER\n",
    "reshaped = melody\n",
    "\n",
    "train_normal(10001, batch_size=16, isMelody=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x[0] for x in d_losses], color='black', linewidth=0.25)\n",
    "plt.plot([x[1] for x in d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[2] for x in d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[0] for x in g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "#plt.xlim(0, 20000)\n",
    "plt.ylim(0, 30)\n",
    "\n",
    "plt.savefig(os.path.join(IMAGE_FOLDER, \"loss_chart2.png\"), format='png')\n",
    "plt.show()\n",
    "plt.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelle: https://stackoverflow.com/a/6537563\n",
    "# Beep to tell training finished\n",
    "import winsound\n",
    "frequency = 300  # Set Frequency To 2500 Hertz\n",
    "duration = 1000  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
