{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.GAN import GAN\n",
    "from utils.loaders import load_safari\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run params\n",
    "SECTION = 'gan'\n",
    "RUN_ID = '0001'\n",
    "DATA_NAME = 'camel'\n",
    "RUN_FOLDER = 'run/{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = load_safari(DATA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x130a12f0e08>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[200,:,:,0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(input_dim = (28,28,1)\n",
    "        , discriminator_conv_filters = [64,64,128,128]\n",
    "        , discriminator_conv_kernel_size = [5,5,5,5]\n",
    "        , discriminator_conv_strides = [2,2,2,1]\n",
    "        , discriminator_batch_norm_momentum = None\n",
    "        , discriminator_activation = 'relu'\n",
    "        , discriminator_dropout_rate = 0.4\n",
    "        , discriminator_learning_rate = 0.0008\n",
    "        , generator_initial_dense_layer_size = (7, 7, 64)\n",
    "        , generator_upsample = [2,2, 1, 1]\n",
    "        , generator_conv_filters = [128,64, 64,1]\n",
    "        , generator_conv_kernel_size = [5,5,5,5]\n",
    "        , generator_conv_strides = [1,1, 1, 1]\n",
    "        , generator_batch_norm_momentum = 0.9\n",
    "        , generator_activation = 'relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.0004\n",
    "        , optimiser = 'rmsprop'\n",
    "        , z_dim = 100\n",
    "        )\n",
    "\n",
    "if mode == 'build':\n",
    "    gan.save(RUN_FOLDER)\n",
    "else:\n",
    "    gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_0 (Conv2D (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_1 (Conv2D (None, 7, 7, 64)          102464    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_2 (Conv2D (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_3 (Conv2D (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 720,833\n",
      "Trainable params: 720,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_0 (Conv2D)    (None, 14, 14, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_1 (Conv2D)    (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_2 (Conv2DTran (None, 28, 28, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_3 (Conv2DTran (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 844,161\n",
      "Trainable params: 837,377\n",
      "Non-trainable params: 6,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 6000\n",
    "PRINT_EVERY_N_BATCHES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pol\\Anaconda3\\envs\\GDL\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: (0.716)(R 0.695, F 0.737)] [D acc: (0.203)(0.406, 0.000)] [G loss: 0.685] [G acc: 1.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pol\\Anaconda3\\envs\\GDL\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: (0.689)(R 0.673, F 0.706)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.546] [G acc: 1.000]\n",
      "2 [D loss: (4.816)(R 0.527, F 9.106)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.552] [G acc: 1.000]\n",
      "3 [D loss: (0.623)(R 0.553, F 0.694)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.509] [G acc: 1.000]\n",
      "4 [D loss: (0.612)(R 0.508, F 0.716)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.484] [G acc: 1.000]\n",
      "5 [D loss: (0.608)(R 0.456, F 0.760)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.458] [G acc: 1.000]\n",
      "6 [D loss: (0.611)(R 0.382, F 0.841)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.449] [G acc: 1.000]\n",
      "7 [D loss: (0.614)(R 0.326, F 0.903)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.466] [G acc: 1.000]\n",
      "8 [D loss: (0.597)(R 0.264, F 0.930)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.504] [G acc: 1.000]\n",
      "9 [D loss: (0.555)(R 0.207, F 0.904)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.544] [G acc: 1.000]\n",
      "10 [D loss: (0.501)(R 0.145, F 0.857)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.587] [G acc: 1.000]\n",
      "11 [D loss: (0.447)(R 0.089, F 0.804)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.628] [G acc: 1.000]\n",
      "12 [D loss: (0.399)(R 0.045, F 0.754)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.658] [G acc: 1.000]\n",
      "13 [D loss: (0.373)(R 0.024, F 0.721)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.679] [G acc: 1.000]\n",
      "14 [D loss: (0.360)(R 0.014, F 0.705)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.691] [G acc: 0.781]\n",
      "15 [D loss: (0.349)(R 0.006, F 0.693)] [D acc: (0.805)(1.000, 0.609)] [G loss: 0.697] [G acc: 0.078]\n",
      "16 [D loss: (0.346)(R 0.003, F 0.689)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.704] [G acc: 0.000]\n",
      "17 [D loss: (0.342)(R 0.002, F 0.682)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.725] [G acc: 0.000]\n",
      "18 [D loss: (0.332)(R 0.001, F 0.663)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.800] [G acc: 0.000]\n",
      "19 [D loss: (0.315)(R 0.000, F 0.629)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.955] [G acc: 0.000]\n",
      "20 [D loss: (0.262)(R 0.001, F 0.522)] [D acc: (1.000)(1.000, 1.000)] [G loss: 1.568] [G acc: 0.000]\n",
      "21 [D loss: (4.649)(R 0.018, F 9.281)] [D acc: (0.500)(1.000, 0.000)] [G loss: 1.516] [G acc: 0.000]\n",
      "22 [D loss: (1.818)(R 1.316, F 2.320)] [D acc: (0.016)(0.031, 0.000)] [G loss: 0.750] [G acc: 0.188]\n",
      "23 [D loss: (0.781)(R 0.470, F 1.092)] [D acc: (0.445)(0.891, 0.000)] [G loss: 0.661] [G acc: 0.688]\n",
      "24 [D loss: (0.725)(R 0.391, F 1.058)] [D acc: (0.492)(0.984, 0.000)] [G loss: 0.647] [G acc: 0.750]\n",
      "25 [D loss: (0.571)(R 0.416, F 0.727)] [D acc: (0.703)(0.969, 0.438)] [G loss: 0.506] [G acc: 0.922]\n",
      "26 [D loss: (0.549)(R 0.381, F 0.718)] [D acc: (0.758)(0.922, 0.594)] [G loss: 0.364] [G acc: 0.969]\n",
      "27 [D loss: (0.376)(R 0.309, F 0.444)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.192] [G acc: 1.000]\n",
      "28 [D loss: (0.240)(R 0.190, F 0.289)] [D acc: (0.984)(0.969, 1.000)] [G loss: 0.093] [G acc: 1.000]\n",
      "29 [D loss: (0.127)(R 0.131, F 0.123)] [D acc: (0.992)(0.984, 1.000)] [G loss: 0.039] [G acc: 1.000]\n",
      "30 [D loss: (0.074)(R 0.085, F 0.062)] [D acc: (0.992)(0.984, 1.000)] [G loss: 0.015] [G acc: 1.000]\n",
      "31 [D loss: (0.054)(R 0.075, F 0.033)] [D acc: (0.992)(0.984, 1.000)] [G loss: 0.007] [G acc: 1.000]\n",
      "32 [D loss: (0.027)(R 0.020, F 0.035)] [D acc: (0.992)(1.000, 0.984)] [G loss: 0.005] [G acc: 1.000]\n",
      "33 [D loss: (0.155)(R 0.039, F 0.272)] [D acc: (0.922)(1.000, 0.844)] [G loss: 0.195] [G acc: 0.938]\n",
      "34 [D loss: (5.449)(R 3.416, F 7.482)] [D acc: (0.109)(0.219, 0.000)] [G loss: 0.686] [G acc: 0.516]\n",
      "35 [D loss: (0.891)(R 0.456, F 1.325)] [D acc: (0.484)(0.969, 0.000)] [G loss: 0.664] [G acc: 0.609]\n",
      "36 [D loss: (0.854)(R 0.499, F 1.210)] [D acc: (0.492)(0.984, 0.000)] [G loss: 0.664] [G acc: 0.688]\n",
      "37 [D loss: (0.786)(R 0.593, F 0.978)] [D acc: (0.445)(0.891, 0.000)] [G loss: 0.634] [G acc: 0.859]\n",
      "38 [D loss: (0.717)(R 0.580, F 0.855)] [D acc: (0.461)(0.922, 0.000)] [G loss: 0.609] [G acc: 0.938]\n",
      "39 [D loss: (0.688)(R 0.582, F 0.794)] [D acc: (0.477)(0.953, 0.000)] [G loss: 0.566] [G acc: 1.000]\n",
      "40 [D loss: (0.647)(R 0.546, F 0.748)] [D acc: (0.586)(1.000, 0.172)] [G loss: 0.495] [G acc: 1.000]\n",
      "41 [D loss: (0.619)(R 0.504, F 0.734)] [D acc: (0.672)(1.000, 0.344)] [G loss: 0.434] [G acc: 1.000]\n",
      "42 [D loss: (0.641)(R 0.451, F 0.830)] [D acc: (0.664)(0.984, 0.344)] [G loss: 0.434] [G acc: 1.000]\n",
      "43 [D loss: (0.716)(R 0.510, F 0.922)] [D acc: (0.602)(1.000, 0.203)] [G loss: 0.488] [G acc: 1.000]\n",
      "44 [D loss: (0.784)(R 0.602, F 0.967)] [D acc: (0.461)(0.906, 0.016)] [G loss: 0.559] [G acc: 1.000]\n",
      "45 [D loss: (0.749)(R 0.636, F 0.862)] [D acc: (0.453)(0.906, 0.000)] [G loss: 0.609] [G acc: 1.000]\n",
      "46 [D loss: (0.733)(R 0.661, F 0.805)] [D acc: (0.422)(0.844, 0.000)] [G loss: 0.648] [G acc: 0.969]\n",
      "47 [D loss: (0.722)(R 0.677, F 0.766)] [D acc: (0.391)(0.781, 0.000)] [G loss: 0.683] [G acc: 0.641]\n",
      "48 [D loss: (0.706)(R 0.690, F 0.721)] [D acc: (0.344)(0.578, 0.109)] [G loss: 0.705] [G acc: 0.297]\n",
      "49 [D loss: (0.704)(R 0.697, F 0.710)] [D acc: (0.328)(0.375, 0.281)] [G loss: 0.724] [G acc: 0.078]\n",
      "50 [D loss: (0.696)(R 0.702, F 0.689)] [D acc: (0.414)(0.266, 0.562)] [G loss: 0.737] [G acc: 0.016]\n",
      "51 [D loss: (0.695)(R 0.711, F 0.678)] [D acc: (0.500)(0.219, 0.781)] [G loss: 0.755] [G acc: 0.016]\n",
      "52 [D loss: (0.692)(R 0.721, F 0.663)] [D acc: (0.570)(0.203, 0.938)] [G loss: 0.781] [G acc: 0.016]\n",
      "53 [D loss: (0.690)(R 0.720, F 0.661)] [D acc: (0.539)(0.156, 0.922)] [G loss: 0.783] [G acc: 0.000]\n",
      "54 [D loss: (0.685)(R 0.713, F 0.658)] [D acc: (0.648)(0.312, 0.984)] [G loss: 0.795] [G acc: 0.000]\n",
      "55 [D loss: (0.665)(R 0.702, F 0.627)] [D acc: (0.680)(0.359, 1.000)] [G loss: 0.849] [G acc: 0.000]\n",
      "56 [D loss: (0.652)(R 0.690, F 0.614)] [D acc: (0.734)(0.469, 1.000)] [G loss: 0.854] [G acc: 0.062]\n",
      "57 [D loss: (0.623)(R 0.648, F 0.599)] [D acc: (0.789)(0.625, 0.953)] [G loss: 0.695] [G acc: 0.484]\n",
      "58 [D loss: (1.620)(R 0.496, F 2.745)] [D acc: (0.453)(0.906, 0.000)] [G loss: 0.722] [G acc: 0.391]\n",
      "59 [D loss: (0.522)(R 0.717, F 0.327)] [D acc: (0.625)(0.250, 1.000)] [G loss: 2.825] [G acc: 0.000]\n",
      "60 [D loss: (0.366)(R 0.681, F 0.051)] [D acc: (0.836)(0.672, 1.000)] [G loss: 0.635] [G acc: 0.781]\n",
      "61 [D loss: (0.376)(R 0.647, F 0.104)] [D acc: (0.898)(0.812, 0.984)] [G loss: 0.542] [G acc: 1.000]\n",
      "62 [D loss: (0.307)(R 0.562, F 0.052)] [D acc: (0.945)(0.906, 0.984)] [G loss: 0.404] [G acc: 1.000]\n",
      "63 [D loss: (0.327)(R 0.487, F 0.168)] [D acc: (0.930)(0.938, 0.922)] [G loss: 0.618] [G acc: 0.766]\n",
      "64 [D loss: (1.315)(R 1.305, F 1.325)] [D acc: (0.211)(0.422, 0.000)] [G loss: 1.138] [G acc: 0.000]\n",
      "65 [D loss: (0.857)(R 1.069, F 0.644)] [D acc: (0.445)(0.000, 0.891)] [G loss: 0.833] [G acc: 0.000]\n",
      "66 [D loss: (0.684)(R 0.735, F 0.632)] [D acc: (0.609)(0.312, 0.906)] [G loss: 0.831] [G acc: 0.016]\n",
      "67 [D loss: (0.677)(R 0.725, F 0.629)] [D acc: (0.609)(0.328, 0.891)] [G loss: 0.839] [G acc: 0.031]\n",
      "68 [D loss: (0.663)(R 0.673, F 0.653)] [D acc: (0.641)(0.625, 0.656)] [G loss: 0.853] [G acc: 0.078]\n",
      "69 [D loss: (0.668)(R 0.695, F 0.641)] [D acc: (0.633)(0.484, 0.781)] [G loss: 0.872] [G acc: 0.062]\n",
      "70 [D loss: (0.670)(R 0.674, F 0.665)] [D acc: (0.648)(0.594, 0.703)] [G loss: 0.862] [G acc: 0.125]\n",
      "71 [D loss: (0.689)(R 0.743, F 0.635)] [D acc: (0.539)(0.297, 0.781)] [G loss: 0.867] [G acc: 0.203]\n",
      "72 [D loss: (0.684)(R 0.686, F 0.681)] [D acc: (0.617)(0.531, 0.703)] [G loss: 0.873] [G acc: 0.094]\n",
      "73 [D loss: (0.711)(R 0.717, F 0.705)] [D acc: (0.453)(0.391, 0.516)] [G loss: 0.805] [G acc: 0.172]\n",
      "74 [D loss: (0.754)(R 0.729, F 0.779)] [D acc: (0.344)(0.312, 0.375)] [G loss: 0.791] [G acc: 0.297]\n",
      "75 [D loss: (0.735)(R 0.719, F 0.750)] [D acc: (0.359)(0.453, 0.266)] [G loss: 0.816] [G acc: 0.172]\n",
      "76 [D loss: (0.556)(R 0.687, F 0.426)] [D acc: (0.664)(0.594, 0.734)] [G loss: 0.876] [G acc: 0.266]\n",
      "77 [D loss: (0.391)(R 0.632, F 0.150)] [D acc: (0.914)(0.859, 0.969)] [G loss: 0.922] [G acc: 0.281]\n",
      "78 [D loss: (0.931)(R 1.116, F 0.745)] [D acc: (0.344)(0.453, 0.234)] [G loss: 0.938] [G acc: 0.047]\n",
      "79 [D loss: (0.706)(R 0.721, F 0.692)] [D acc: (0.500)(0.453, 0.547)] [G loss: 0.835] [G acc: 0.141]\n",
      "80 [D loss: (0.700)(R 0.690, F 0.711)] [D acc: (0.523)(0.625, 0.422)] [G loss: 0.756] [G acc: 0.344]\n",
      "81 [D loss: (0.671)(R 0.626, F 0.717)] [D acc: (0.633)(0.781, 0.484)] [G loss: 0.734] [G acc: 0.359]\n",
      "82 [D loss: (0.693)(R 0.648, F 0.737)] [D acc: (0.461)(0.625, 0.297)] [G loss: 0.694] [G acc: 0.531]\n",
      "83 [D loss: (0.688)(R 0.618, F 0.759)] [D acc: (0.555)(0.812, 0.297)] [G loss: 0.697] [G acc: 0.547]\n",
      "84 [D loss: (0.665)(R 0.572, F 0.759)] [D acc: (0.641)(0.938, 0.344)] [G loss: 0.733] [G acc: 0.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 [D loss: (0.688)(R 0.559, F 0.816)] [D acc: (0.602)(0.953, 0.250)] [G loss: 0.773] [G acc: 0.219]\n",
      "86 [D loss: (0.669)(R 0.530, F 0.808)] [D acc: (0.578)(0.969, 0.188)] [G loss: 0.765] [G acc: 0.156]\n",
      "87 [D loss: (0.669)(R 0.560, F 0.778)] [D acc: (0.578)(0.875, 0.281)] [G loss: 0.799] [G acc: 0.031]\n",
      "88 [D loss: (0.658)(R 0.554, F 0.762)] [D acc: (0.555)(0.797, 0.312)] [G loss: 0.827] [G acc: 0.047]\n",
      "89 [D loss: (0.675)(R 0.513, F 0.838)] [D acc: (0.516)(0.859, 0.172)] [G loss: 0.859] [G acc: 0.094]\n",
      "90 [D loss: (0.610)(R 0.552, F 0.668)] [D acc: (0.625)(0.750, 0.500)] [G loss: 0.762] [G acc: 0.234]\n",
      "91 [D loss: (0.668)(R 0.696, F 0.640)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.093] [G acc: 0.016]\n",
      "92 [D loss: (0.752)(R 0.600, F 0.904)] [D acc: (0.438)(0.703, 0.172)] [G loss: 0.711] [G acc: 0.422]\n",
      "93 [D loss: (0.625)(R 0.470, F 0.781)] [D acc: (0.578)(0.859, 0.297)] [G loss: 0.725] [G acc: 0.281]\n",
      "94 [D loss: (0.610)(R 0.418, F 0.803)] [D acc: (0.602)(0.938, 0.266)] [G loss: 0.764] [G acc: 0.109]\n",
      "95 [D loss: (0.545)(R 0.448, F 0.641)] [D acc: (0.750)(0.859, 0.641)] [G loss: 0.911] [G acc: 0.078]\n",
      "96 [D loss: (0.631)(R 0.539, F 0.723)] [D acc: (0.633)(0.766, 0.500)] [G loss: 1.194] [G acc: 0.000]\n",
      "97 [D loss: (0.579)(R 0.456, F 0.702)] [D acc: (0.648)(0.828, 0.469)] [G loss: 0.850] [G acc: 0.172]\n",
      "98 [D loss: (0.326)(R 0.304, F 0.348)] [D acc: (0.961)(0.984, 0.938)] [G loss: 1.206] [G acc: 0.141]\n",
      "99 [D loss: (0.477)(R 0.583, F 0.372)] [D acc: (0.859)(0.781, 0.938)] [G loss: 2.403] [G acc: 0.016]\n",
      "100 [D loss: (0.548)(R 0.125, F 0.971)] [D acc: (0.680)(0.953, 0.406)] [G loss: 1.979] [G acc: 0.000]\n",
      "101 [D loss: (0.400)(R 0.738, F 0.062)] [D acc: (0.742)(0.484, 1.000)] [G loss: 0.854] [G acc: 0.219]\n",
      "102 [D loss: (0.322)(R 0.210, F 0.434)] [D acc: (0.898)(0.953, 0.844)] [G loss: 1.221] [G acc: 0.094]\n",
      "103 [D loss: (0.380)(R 0.208, F 0.552)] [D acc: (0.883)(0.891, 0.875)] [G loss: 1.408] [G acc: 0.016]\n",
      "104 [D loss: (0.349)(R 0.089, F 0.609)] [D acc: (0.844)(0.969, 0.719)] [G loss: 1.480] [G acc: 0.000]\n",
      "105 [D loss: (0.430)(R 0.205, F 0.655)] [D acc: (0.781)(0.906, 0.656)] [G loss: 1.700] [G acc: 0.000]\n",
      "106 [D loss: (0.490)(R 0.506, F 0.474)] [D acc: (0.797)(0.750, 0.844)] [G loss: 2.705] [G acc: 0.000]\n",
      "107 [D loss: (0.333)(R 0.429, F 0.236)] [D acc: (0.875)(0.812, 0.938)] [G loss: 2.889] [G acc: 0.000]\n",
      "108 [D loss: (0.536)(R 0.294, F 0.777)] [D acc: (0.773)(0.891, 0.656)] [G loss: 2.007] [G acc: 0.000]\n",
      "109 [D loss: (0.650)(R 0.788, F 0.512)] [D acc: (0.633)(0.453, 0.812)] [G loss: 2.883] [G acc: 0.000]\n",
      "110 [D loss: (0.384)(R 0.405, F 0.364)] [D acc: (0.883)(0.844, 0.922)] [G loss: 1.293] [G acc: 0.141]\n",
      "111 [D loss: (0.543)(R 0.308, F 0.779)] [D acc: (0.742)(0.875, 0.609)] [G loss: 1.347] [G acc: 0.016]\n",
      "112 [D loss: (0.817)(R 0.891, F 0.743)] [D acc: (0.430)(0.406, 0.453)] [G loss: 1.410] [G acc: 0.016]\n",
      "113 [D loss: (0.591)(R 0.565, F 0.616)] [D acc: (0.758)(0.797, 0.719)] [G loss: 1.029] [G acc: 0.125]\n",
      "114 [D loss: (0.674)(R 0.547, F 0.801)] [D acc: (0.570)(0.688, 0.453)] [G loss: 0.950] [G acc: 0.094]\n",
      "115 [D loss: (0.663)(R 0.613, F 0.712)] [D acc: (0.641)(0.703, 0.578)] [G loss: 1.170] [G acc: 0.031]\n",
      "116 [D loss: (0.588)(R 0.538, F 0.638)] [D acc: (0.727)(0.781, 0.672)] [G loss: 1.069] [G acc: 0.062]\n",
      "117 [D loss: (0.659)(R 0.543, F 0.775)] [D acc: (0.570)(0.719, 0.422)] [G loss: 1.053] [G acc: 0.000]\n",
      "118 [D loss: (0.586)(R 0.548, F 0.624)] [D acc: (0.727)(0.750, 0.703)] [G loss: 1.196] [G acc: 0.016]\n",
      "119 [D loss: (0.768)(R 0.736, F 0.800)] [D acc: (0.516)(0.547, 0.484)] [G loss: 0.916] [G acc: 0.047]\n",
      "120 [D loss: (0.661)(R 0.625, F 0.696)] [D acc: (0.617)(0.703, 0.531)] [G loss: 0.886] [G acc: 0.016]\n",
      "121 [D loss: (0.635)(R 0.560, F 0.711)] [D acc: (0.633)(0.703, 0.562)] [G loss: 0.978] [G acc: 0.016]\n",
      "122 [D loss: (0.590)(R 0.513, F 0.666)] [D acc: (0.664)(0.734, 0.594)] [G loss: 1.226] [G acc: 0.000]\n",
      "123 [D loss: (0.594)(R 0.539, F 0.648)] [D acc: (0.727)(0.766, 0.688)] [G loss: 1.220] [G acc: 0.000]\n",
      "124 [D loss: (0.521)(R 0.402, F 0.640)] [D acc: (0.727)(0.781, 0.672)] [G loss: 1.726] [G acc: 0.000]\n",
      "125 [D loss: (0.709)(R 0.703, F 0.715)] [D acc: (0.586)(0.594, 0.578)] [G loss: 1.045] [G acc: 0.000]\n",
      "126 [D loss: (0.510)(R 0.395, F 0.626)] [D acc: (0.812)(0.875, 0.750)] [G loss: 1.206] [G acc: 0.016]\n",
      "127 [D loss: (0.501)(R 0.417, F 0.585)] [D acc: (0.797)(0.844, 0.750)] [G loss: 1.441] [G acc: 0.000]\n",
      "128 [D loss: (0.430)(R 0.305, F 0.555)] [D acc: (0.867)(0.906, 0.828)] [G loss: 1.874] [G acc: 0.000]\n",
      "129 [D loss: (0.439)(R 0.347, F 0.532)] [D acc: (0.812)(0.891, 0.734)] [G loss: 1.834] [G acc: 0.000]\n",
      "130 [D loss: (0.479)(R 0.388, F 0.571)] [D acc: (0.773)(0.828, 0.719)] [G loss: 2.027] [G acc: 0.000]\n",
      "131 [D loss: (0.277)(R 0.286, F 0.268)] [D acc: (0.953)(0.922, 0.984)] [G loss: 1.412] [G acc: 0.047]\n",
      "132 [D loss: (0.680)(R 0.109, F 1.252)] [D acc: (0.602)(0.953, 0.250)] [G loss: 2.602] [G acc: 0.000]\n",
      "133 [D loss: (0.644)(R 1.028, F 0.260)] [D acc: (0.656)(0.328, 0.984)] [G loss: 1.165] [G acc: 0.000]\n",
      "134 [D loss: (0.448)(R 0.241, F 0.654)] [D acc: (0.852)(0.938, 0.766)] [G loss: 1.299] [G acc: 0.000]\n",
      "135 [D loss: (0.399)(R 0.281, F 0.517)] [D acc: (0.867)(0.922, 0.812)] [G loss: 1.389] [G acc: 0.016]\n",
      "136 [D loss: (0.414)(R 0.301, F 0.527)] [D acc: (0.867)(0.891, 0.844)] [G loss: 1.692] [G acc: 0.000]\n",
      "137 [D loss: (0.388)(R 0.190, F 0.587)] [D acc: (0.828)(0.922, 0.734)] [G loss: 2.303] [G acc: 0.000]\n",
      "138 [D loss: (0.561)(R 0.525, F 0.596)] [D acc: (0.719)(0.750, 0.688)] [G loss: 2.281] [G acc: 0.000]\n",
      "139 [D loss: (0.504)(R 0.560, F 0.448)] [D acc: (0.773)(0.734, 0.812)] [G loss: 2.215] [G acc: 0.000]\n",
      "140 [D loss: (0.397)(R 0.182, F 0.612)] [D acc: (0.844)(0.938, 0.750)] [G loss: 1.806] [G acc: 0.000]\n",
      "141 [D loss: (0.631)(R 0.690, F 0.571)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.527] [G acc: 0.000]\n",
      "142 [D loss: (0.420)(R 0.302, F 0.537)] [D acc: (0.891)(0.922, 0.859)] [G loss: 1.373] [G acc: 0.016]\n",
      "143 [D loss: (0.459)(R 0.265, F 0.653)] [D acc: (0.797)(0.922, 0.672)] [G loss: 1.519] [G acc: 0.000]\n",
      "144 [D loss: (0.530)(R 0.495, F 0.564)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.413] [G acc: 0.000]\n",
      "145 [D loss: (0.390)(R 0.277, F 0.504)] [D acc: (0.883)(0.891, 0.875)] [G loss: 1.469] [G acc: 0.000]\n",
      "146 [D loss: (0.433)(R 0.268, F 0.598)] [D acc: (0.773)(0.891, 0.656)] [G loss: 1.534] [G acc: 0.016]\n",
      "147 [D loss: (0.413)(R 0.291, F 0.534)] [D acc: (0.844)(0.875, 0.812)] [G loss: 1.716] [G acc: 0.000]\n",
      "148 [D loss: (1.113)(R 0.341, F 1.885)] [D acc: (0.531)(0.812, 0.250)] [G loss: 1.870] [G acc: 0.000]\n",
      "149 [D loss: (0.707)(R 0.925, F 0.488)] [D acc: (0.641)(0.328, 0.953)] [G loss: 0.967] [G acc: 0.203]\n",
      "150 [D loss: (0.720)(R 0.517, F 0.923)] [D acc: (0.570)(0.750, 0.391)] [G loss: 0.821] [G acc: 0.172]\n",
      "151 [D loss: (0.791)(R 0.645, F 0.936)] [D acc: (0.484)(0.625, 0.344)] [G loss: 0.735] [G acc: 0.297]\n",
      "152 [D loss: (0.802)(R 0.560, F 1.043)] [D acc: (0.516)(0.734, 0.297)] [G loss: 0.777] [G acc: 0.219]\n",
      "153 [D loss: (0.726)(R 0.701, F 0.752)] [D acc: (0.531)(0.469, 0.594)] [G loss: 0.760] [G acc: 0.219]\n",
      "154 [D loss: (0.703)(R 0.626, F 0.781)] [D acc: (0.656)(0.688, 0.625)] [G loss: 0.756] [G acc: 0.297]\n",
      "155 [D loss: (0.705)(R 0.635, F 0.776)] [D acc: (0.586)(0.656, 0.516)] [G loss: 0.770] [G acc: 0.219]\n",
      "156 [D loss: (0.674)(R 0.657, F 0.691)] [D acc: (0.609)(0.578, 0.641)] [G loss: 0.760] [G acc: 0.312]\n",
      "157 [D loss: (0.679)(R 0.659, F 0.698)] [D acc: (0.555)(0.547, 0.562)] [G loss: 0.739] [G acc: 0.312]\n",
      "158 [D loss: (0.724)(R 0.645, F 0.802)] [D acc: (0.422)(0.547, 0.297)] [G loss: 0.726] [G acc: 0.297]\n",
      "159 [D loss: (0.692)(R 0.614, F 0.769)] [D acc: (0.539)(0.688, 0.391)] [G loss: 0.718] [G acc: 0.234]\n",
      "160 [D loss: (0.664)(R 0.633, F 0.696)] [D acc: (0.672)(0.781, 0.562)] [G loss: 0.723] [G acc: 0.250]\n",
      "161 [D loss: (0.687)(R 0.614, F 0.759)] [D acc: (0.562)(0.734, 0.391)] [G loss: 0.719] [G acc: 0.297]\n",
      "162 [D loss: (0.697)(R 0.634, F 0.760)] [D acc: (0.586)(0.750, 0.422)] [G loss: 0.741] [G acc: 0.312]\n",
      "163 [D loss: (0.682)(R 0.653, F 0.711)] [D acc: (0.602)(0.703, 0.500)] [G loss: 0.727] [G acc: 0.219]\n",
      "164 [D loss: (0.665)(R 0.599, F 0.731)] [D acc: (0.609)(0.797, 0.422)] [G loss: 0.734] [G acc: 0.281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 [D loss: (0.662)(R 0.597, F 0.726)] [D acc: (0.602)(0.734, 0.469)] [G loss: 0.736] [G acc: 0.250]\n",
      "166 [D loss: (0.724)(R 0.557, F 0.892)] [D acc: (0.555)(0.781, 0.328)] [G loss: 0.710] [G acc: 0.359]\n",
      "167 [D loss: (0.687)(R 0.653, F 0.721)] [D acc: (0.531)(0.656, 0.406)] [G loss: 0.696] [G acc: 0.438]\n",
      "168 [D loss: (0.700)(R 0.597, F 0.804)] [D acc: (0.516)(0.812, 0.219)] [G loss: 0.714] [G acc: 0.453]\n",
      "169 [D loss: (0.718)(R 0.649, F 0.788)] [D acc: (0.414)(0.641, 0.188)] [G loss: 0.699] [G acc: 0.516]\n",
      "170 [D loss: (0.712)(R 0.656, F 0.768)] [D acc: (0.406)(0.656, 0.156)] [G loss: 0.689] [G acc: 0.422]\n",
      "171 [D loss: (0.686)(R 0.634, F 0.738)] [D acc: (0.578)(0.797, 0.359)] [G loss: 0.727] [G acc: 0.266]\n",
      "172 [D loss: (0.708)(R 0.621, F 0.795)] [D acc: (0.539)(0.750, 0.328)] [G loss: 0.780] [G acc: 0.125]\n",
      "173 [D loss: (0.658)(R 0.690, F 0.626)] [D acc: (0.578)(0.500, 0.656)] [G loss: 0.749] [G acc: 0.203]\n",
      "174 [D loss: (0.735)(R 0.680, F 0.791)] [D acc: (0.430)(0.531, 0.328)] [G loss: 0.736] [G acc: 0.188]\n",
      "175 [D loss: (0.679)(R 0.673, F 0.685)] [D acc: (0.578)(0.609, 0.547)] [G loss: 0.697] [G acc: 0.422]\n",
      "176 [D loss: (0.647)(R 0.661, F 0.634)] [D acc: (0.688)(0.781, 0.594)] [G loss: 0.705] [G acc: 0.453]\n",
      "177 [D loss: (0.688)(R 0.678, F 0.698)] [D acc: (0.500)(0.578, 0.422)] [G loss: 0.673] [G acc: 0.578]\n",
      "178 [D loss: (0.662)(R 0.623, F 0.702)] [D acc: (0.703)(0.891, 0.516)] [G loss: 0.700] [G acc: 0.453]\n",
      "179 [D loss: (0.695)(R 0.676, F 0.714)] [D acc: (0.547)(0.625, 0.469)] [G loss: 0.688] [G acc: 0.484]\n",
      "180 [D loss: (0.664)(R 0.597, F 0.731)] [D acc: (0.641)(0.812, 0.469)] [G loss: 0.768] [G acc: 0.281]\n",
      "181 [D loss: (0.628)(R 0.632, F 0.624)] [D acc: (0.695)(0.703, 0.688)] [G loss: 0.644] [G acc: 0.641]\n",
      "182 [D loss: (0.789)(R 0.653, F 0.925)] [D acc: (0.438)(0.719, 0.156)] [G loss: 0.731] [G acc: 0.328]\n",
      "183 [D loss: (0.679)(R 0.622, F 0.736)] [D acc: (0.625)(0.891, 0.359)] [G loss: 0.777] [G acc: 0.234]\n",
      "184 [D loss: (0.591)(R 0.594, F 0.587)] [D acc: (0.789)(0.875, 0.703)] [G loss: 0.757] [G acc: 0.422]\n",
      "185 [D loss: (0.670)(R 0.642, F 0.698)] [D acc: (0.570)(0.609, 0.531)] [G loss: 1.283] [G acc: 0.031]\n",
      "186 [D loss: (0.362)(R 0.497, F 0.227)] [D acc: (0.891)(0.812, 0.969)] [G loss: 1.041] [G acc: 0.312]\n",
      "187 [D loss: (0.353)(R 0.275, F 0.430)] [D acc: (0.930)(0.984, 0.875)] [G loss: 2.982] [G acc: 0.031]\n",
      "188 [D loss: (1.224)(R 0.350, F 2.098)] [D acc: (0.445)(0.875, 0.016)] [G loss: 0.722] [G acc: 0.375]\n",
      "189 [D loss: (0.633)(R 0.453, F 0.813)] [D acc: (0.586)(0.844, 0.328)] [G loss: 0.753] [G acc: 0.438]\n",
      "190 [D loss: (0.695)(R 0.568, F 0.822)] [D acc: (0.609)(0.797, 0.422)] [G loss: 0.694] [G acc: 0.516]\n",
      "191 [D loss: (0.660)(R 0.491, F 0.828)] [D acc: (0.586)(0.891, 0.281)] [G loss: 0.735] [G acc: 0.344]\n",
      "192 [D loss: (0.673)(R 0.507, F 0.838)] [D acc: (0.586)(0.828, 0.344)] [G loss: 0.759] [G acc: 0.344]\n",
      "193 [D loss: (0.661)(R 0.565, F 0.757)] [D acc: (0.547)(0.719, 0.375)] [G loss: 0.759] [G acc: 0.219]\n",
      "194 [D loss: (0.727)(R 0.624, F 0.829)] [D acc: (0.516)(0.703, 0.328)] [G loss: 0.730] [G acc: 0.312]\n",
      "195 [D loss: (0.663)(R 0.576, F 0.750)] [D acc: (0.664)(0.781, 0.547)] [G loss: 0.738] [G acc: 0.344]\n",
      "196 [D loss: (0.663)(R 0.600, F 0.726)] [D acc: (0.594)(0.672, 0.516)] [G loss: 0.758] [G acc: 0.219]\n",
      "197 [D loss: (0.671)(R 0.569, F 0.773)] [D acc: (0.586)(0.812, 0.359)] [G loss: 0.778] [G acc: 0.219]\n",
      "198 [D loss: (0.691)(R 0.622, F 0.759)] [D acc: (0.516)(0.688, 0.344)] [G loss: 0.759] [G acc: 0.156]\n",
      "199 [D loss: (0.665)(R 0.592, F 0.737)] [D acc: (0.594)(0.766, 0.422)] [G loss: 0.762] [G acc: 0.203]\n",
      "200 [D loss: (0.683)(R 0.606, F 0.759)] [D acc: (0.516)(0.703, 0.328)] [G loss: 0.763] [G acc: 0.172]\n",
      "201 [D loss: (0.665)(R 0.620, F 0.711)] [D acc: (0.586)(0.688, 0.484)] [G loss: 0.765] [G acc: 0.250]\n",
      "202 [D loss: (0.703)(R 0.624, F 0.782)] [D acc: (0.586)(0.766, 0.406)] [G loss: 0.762] [G acc: 0.141]\n",
      "203 [D loss: (0.665)(R 0.627, F 0.703)] [D acc: (0.625)(0.703, 0.547)] [G loss: 0.777] [G acc: 0.203]\n",
      "204 [D loss: (0.683)(R 0.646, F 0.721)] [D acc: (0.633)(0.766, 0.500)] [G loss: 0.726] [G acc: 0.391]\n",
      "205 [D loss: (0.650)(R 0.610, F 0.691)] [D acc: (0.703)(0.828, 0.578)] [G loss: 0.759] [G acc: 0.141]\n",
      "206 [D loss: (0.693)(R 0.605, F 0.780)] [D acc: (0.562)(0.750, 0.375)] [G loss: 0.724] [G acc: 0.328]\n",
      "207 [D loss: (0.661)(R 0.592, F 0.730)] [D acc: (0.680)(0.906, 0.453)] [G loss: 0.767] [G acc: 0.219]\n",
      "208 [D loss: (0.663)(R 0.596, F 0.730)] [D acc: (0.586)(0.750, 0.422)] [G loss: 0.755] [G acc: 0.328]\n",
      "209 [D loss: (0.672)(R 0.603, F 0.740)] [D acc: (0.586)(0.797, 0.375)] [G loss: 0.749] [G acc: 0.281]\n",
      "210 [D loss: (0.644)(R 0.550, F 0.739)] [D acc: (0.648)(0.844, 0.453)] [G loss: 0.707] [G acc: 0.438]\n",
      "211 [D loss: (0.678)(R 0.569, F 0.786)] [D acc: (0.625)(0.875, 0.375)] [G loss: 0.737] [G acc: 0.359]\n",
      "212 [D loss: (0.688)(R 0.594, F 0.781)] [D acc: (0.547)(0.750, 0.344)] [G loss: 0.740] [G acc: 0.297]\n",
      "213 [D loss: (0.661)(R 0.586, F 0.737)] [D acc: (0.633)(0.844, 0.422)] [G loss: 0.745] [G acc: 0.234]\n",
      "214 [D loss: (0.664)(R 0.550, F 0.778)] [D acc: (0.586)(0.812, 0.359)] [G loss: 0.768] [G acc: 0.203]\n",
      "215 [D loss: (0.682)(R 0.522, F 0.843)] [D acc: (0.555)(0.891, 0.219)] [G loss: 0.818] [G acc: 0.031]\n",
      "216 [D loss: (0.687)(R 0.648, F 0.725)] [D acc: (0.531)(0.594, 0.469)] [G loss: 0.756] [G acc: 0.156]\n",
      "217 [D loss: (0.670)(R 0.620, F 0.720)] [D acc: (0.555)(0.641, 0.469)] [G loss: 0.793] [G acc: 0.094]\n",
      "218 [D loss: (0.673)(R 0.604, F 0.741)] [D acc: (0.586)(0.734, 0.438)] [G loss: 0.780] [G acc: 0.109]\n",
      "219 [D loss: (0.663)(R 0.613, F 0.713)] [D acc: (0.609)(0.688, 0.531)] [G loss: 0.752] [G acc: 0.172]\n",
      "220 [D loss: (0.687)(R 0.575, F 0.799)] [D acc: (0.523)(0.766, 0.281)] [G loss: 0.802] [G acc: 0.094]\n",
      "221 [D loss: (0.685)(R 0.627, F 0.744)] [D acc: (0.539)(0.625, 0.453)] [G loss: 0.761] [G acc: 0.156]\n",
      "222 [D loss: (0.670)(R 0.623, F 0.716)] [D acc: (0.617)(0.719, 0.516)] [G loss: 0.781] [G acc: 0.109]\n",
      "223 [D loss: (0.713)(R 0.593, F 0.833)] [D acc: (0.531)(0.750, 0.312)] [G loss: 0.762] [G acc: 0.031]\n",
      "224 [D loss: (0.686)(R 0.675, F 0.698)] [D acc: (0.539)(0.531, 0.547)] [G loss: 0.758] [G acc: 0.141]\n",
      "225 [D loss: (0.677)(R 0.662, F 0.693)] [D acc: (0.555)(0.578, 0.531)] [G loss: 0.742] [G acc: 0.156]\n",
      "226 [D loss: (0.683)(R 0.630, F 0.735)] [D acc: (0.578)(0.766, 0.391)] [G loss: 0.742] [G acc: 0.141]\n",
      "227 [D loss: (0.655)(R 0.611, F 0.699)] [D acc: (0.633)(0.719, 0.547)] [G loss: 0.821] [G acc: 0.047]\n",
      "228 [D loss: (0.681)(R 0.598, F 0.763)] [D acc: (0.555)(0.703, 0.406)] [G loss: 0.787] [G acc: 0.047]\n",
      "229 [D loss: (0.647)(R 0.619, F 0.676)] [D acc: (0.641)(0.672, 0.609)] [G loss: 0.922] [G acc: 0.016]\n",
      "230 [D loss: (0.761)(R 0.624, F 0.897)] [D acc: (0.398)(0.594, 0.203)] [G loss: 0.790] [G acc: 0.016]\n",
      "231 [D loss: (0.701)(R 0.726, F 0.676)] [D acc: (0.516)(0.375, 0.656)] [G loss: 0.779] [G acc: 0.031]\n",
      "232 [D loss: (0.686)(R 0.642, F 0.729)] [D acc: (0.523)(0.656, 0.391)] [G loss: 0.764] [G acc: 0.047]\n",
      "233 [D loss: (0.684)(R 0.653, F 0.716)] [D acc: (0.578)(0.672, 0.484)] [G loss: 0.761] [G acc: 0.094]\n",
      "234 [D loss: (0.679)(R 0.610, F 0.747)] [D acc: (0.609)(0.781, 0.438)] [G loss: 0.756] [G acc: 0.125]\n",
      "235 [D loss: (0.689)(R 0.650, F 0.727)] [D acc: (0.539)(0.625, 0.453)] [G loss: 0.759] [G acc: 0.078]\n",
      "236 [D loss: (0.682)(R 0.644, F 0.720)] [D acc: (0.562)(0.641, 0.484)] [G loss: 0.752] [G acc: 0.078]\n",
      "237 [D loss: (0.680)(R 0.619, F 0.740)] [D acc: (0.578)(0.688, 0.469)] [G loss: 0.752] [G acc: 0.109]\n",
      "238 [D loss: (0.667)(R 0.627, F 0.706)] [D acc: (0.594)(0.594, 0.594)] [G loss: 0.744] [G acc: 0.125]\n",
      "239 [D loss: (0.641)(R 0.599, F 0.682)] [D acc: (0.680)(0.719, 0.641)] [G loss: 0.774] [G acc: 0.047]\n",
      "240 [D loss: (0.687)(R 0.564, F 0.810)] [D acc: (0.523)(0.688, 0.359)] [G loss: 0.780] [G acc: 0.000]\n",
      "241 [D loss: (0.650)(R 0.648, F 0.652)] [D acc: (0.695)(0.594, 0.797)] [G loss: 0.900] [G acc: 0.016]\n",
      "242 [D loss: (0.807)(R 0.609, F 1.006)] [D acc: (0.438)(0.672, 0.203)] [G loss: 0.788] [G acc: 0.094]\n",
      "243 [D loss: (0.655)(R 0.683, F 0.627)] [D acc: (0.703)(0.609, 0.797)] [G loss: 0.824] [G acc: 0.062]\n",
      "244 [D loss: (0.667)(R 0.636, F 0.698)] [D acc: (0.578)(0.688, 0.469)] [G loss: 0.781] [G acc: 0.109]\n",
      "245 [D loss: (0.649)(R 0.585, F 0.713)] [D acc: (0.641)(0.812, 0.469)] [G loss: 0.792] [G acc: 0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246 [D loss: (0.647)(R 0.571, F 0.724)] [D acc: (0.586)(0.656, 0.516)] [G loss: 0.828] [G acc: 0.141]\n",
      "247 [D loss: (0.675)(R 0.578, F 0.773)] [D acc: (0.641)(0.688, 0.594)] [G loss: 0.934] [G acc: 0.031]\n",
      "248 [D loss: (0.684)(R 0.711, F 0.658)] [D acc: (0.516)(0.359, 0.672)] [G loss: 0.830] [G acc: 0.109]\n",
      "249 [D loss: (0.631)(R 0.536, F 0.726)] [D acc: (0.641)(0.844, 0.438)] [G loss: 0.801] [G acc: 0.172]\n",
      "250 [D loss: (0.636)(R 0.497, F 0.776)] [D acc: (0.688)(0.797, 0.578)] [G loss: 0.797] [G acc: 0.234]\n",
      "251 [D loss: (0.658)(R 0.560, F 0.756)] [D acc: (0.586)(0.688, 0.484)] [G loss: 0.797] [G acc: 0.188]\n",
      "252 [D loss: (0.755)(R 0.617, F 0.894)] [D acc: (0.508)(0.641, 0.375)] [G loss: 0.851] [G acc: 0.141]\n",
      "253 [D loss: (0.674)(R 0.650, F 0.699)] [D acc: (0.602)(0.594, 0.609)] [G loss: 0.779] [G acc: 0.203]\n",
      "254 [D loss: (0.660)(R 0.595, F 0.725)] [D acc: (0.625)(0.688, 0.562)] [G loss: 0.733] [G acc: 0.375]\n",
      "255 [D loss: (0.684)(R 0.598, F 0.769)] [D acc: (0.547)(0.688, 0.406)] [G loss: 0.800] [G acc: 0.188]\n",
      "256 [D loss: (0.710)(R 0.645, F 0.774)] [D acc: (0.555)(0.609, 0.500)] [G loss: 0.892] [G acc: 0.094]\n",
      "257 [D loss: (0.676)(R 0.657, F 0.695)] [D acc: (0.578)(0.547, 0.609)] [G loss: 0.805] [G acc: 0.141]\n",
      "258 [D loss: (0.662)(R 0.624, F 0.699)] [D acc: (0.602)(0.672, 0.531)] [G loss: 0.807] [G acc: 0.172]\n",
      "259 [D loss: (0.673)(R 0.613, F 0.734)] [D acc: (0.602)(0.656, 0.547)] [G loss: 0.807] [G acc: 0.141]\n",
      "260 [D loss: (0.651)(R 0.639, F 0.662)] [D acc: (0.680)(0.656, 0.703)] [G loss: 0.820] [G acc: 0.125]\n",
      "261 [D loss: (0.693)(R 0.652, F 0.733)] [D acc: (0.469)(0.484, 0.453)] [G loss: 0.978] [G acc: 0.016]\n",
      "262 [D loss: (0.676)(R 0.704, F 0.648)] [D acc: (0.641)(0.516, 0.766)] [G loss: 0.782] [G acc: 0.328]\n",
      "263 [D loss: (0.677)(R 0.586, F 0.768)] [D acc: (0.578)(0.781, 0.375)] [G loss: 0.803] [G acc: 0.203]\n",
      "264 [D loss: (0.673)(R 0.628, F 0.718)] [D acc: (0.609)(0.641, 0.578)] [G loss: 0.795] [G acc: 0.188]\n",
      "265 [D loss: (0.647)(R 0.616, F 0.678)] [D acc: (0.711)(0.750, 0.672)] [G loss: 0.835] [G acc: 0.188]\n",
      "266 [D loss: (0.699)(R 0.624, F 0.773)] [D acc: (0.523)(0.625, 0.422)] [G loss: 0.841] [G acc: 0.141]\n",
      "267 [D loss: (0.701)(R 0.686, F 0.716)] [D acc: (0.500)(0.484, 0.516)] [G loss: 0.873] [G acc: 0.062]\n",
      "268 [D loss: (0.687)(R 0.678, F 0.695)] [D acc: (0.523)(0.484, 0.562)] [G loss: 0.805] [G acc: 0.141]\n",
      "269 [D loss: (0.665)(R 0.625, F 0.705)] [D acc: (0.586)(0.703, 0.469)] [G loss: 0.833] [G acc: 0.141]\n",
      "270 [D loss: (0.656)(R 0.654, F 0.657)] [D acc: (0.695)(0.641, 0.750)] [G loss: 0.837] [G acc: 0.141]\n",
      "271 [D loss: (0.673)(R 0.655, F 0.692)] [D acc: (0.555)(0.562, 0.547)] [G loss: 0.848] [G acc: 0.094]\n",
      "272 [D loss: (0.680)(R 0.681, F 0.680)] [D acc: (0.516)(0.531, 0.500)] [G loss: 0.829] [G acc: 0.047]\n",
      "273 [D loss: (0.662)(R 0.663, F 0.661)] [D acc: (0.625)(0.609, 0.641)] [G loss: 0.813] [G acc: 0.141]\n",
      "274 [D loss: (0.663)(R 0.633, F 0.693)] [D acc: (0.594)(0.656, 0.531)] [G loss: 0.843] [G acc: 0.219]\n",
      "275 [D loss: (0.687)(R 0.661, F 0.714)] [D acc: (0.523)(0.578, 0.469)] [G loss: 0.869] [G acc: 0.078]\n",
      "276 [D loss: (0.676)(R 0.695, F 0.657)] [D acc: (0.547)(0.500, 0.594)] [G loss: 0.942] [G acc: 0.062]\n",
      "277 [D loss: (0.666)(R 0.692, F 0.640)] [D acc: (0.617)(0.484, 0.750)] [G loss: 0.922] [G acc: 0.062]\n",
      "278 [D loss: (0.720)(R 0.669, F 0.771)] [D acc: (0.469)(0.547, 0.391)] [G loss: 0.808] [G acc: 0.125]\n",
      "279 [D loss: (0.680)(R 0.635, F 0.725)] [D acc: (0.547)(0.594, 0.500)] [G loss: 0.796] [G acc: 0.172]\n",
      "280 [D loss: (0.650)(R 0.618, F 0.682)] [D acc: (0.633)(0.688, 0.578)] [G loss: 0.806] [G acc: 0.203]\n",
      "281 [D loss: (0.645)(R 0.606, F 0.683)] [D acc: (0.641)(0.672, 0.609)] [G loss: 0.855] [G acc: 0.156]\n",
      "282 [D loss: (0.677)(R 0.614, F 0.740)] [D acc: (0.547)(0.672, 0.422)] [G loss: 0.801] [G acc: 0.188]\n",
      "283 [D loss: (0.649)(R 0.629, F 0.668)] [D acc: (0.641)(0.641, 0.641)] [G loss: 0.805] [G acc: 0.297]\n",
      "284 [D loss: (0.681)(R 0.652, F 0.710)] [D acc: (0.570)(0.625, 0.516)] [G loss: 0.853] [G acc: 0.188]\n",
      "285 [D loss: (0.679)(R 0.625, F 0.733)] [D acc: (0.555)(0.625, 0.484)] [G loss: 0.879] [G acc: 0.156]\n",
      "286 [D loss: (0.676)(R 0.669, F 0.683)] [D acc: (0.555)(0.531, 0.578)] [G loss: 0.836] [G acc: 0.156]\n",
      "287 [D loss: (0.698)(R 0.633, F 0.763)] [D acc: (0.578)(0.703, 0.453)] [G loss: 0.830] [G acc: 0.125]\n",
      "288 [D loss: (0.654)(R 0.627, F 0.682)] [D acc: (0.625)(0.625, 0.625)] [G loss: 0.823] [G acc: 0.156]\n",
      "289 [D loss: (0.655)(R 0.624, F 0.685)] [D acc: (0.656)(0.734, 0.578)] [G loss: 0.799] [G acc: 0.234]\n",
      "290 [D loss: (0.675)(R 0.626, F 0.725)] [D acc: (0.602)(0.688, 0.516)] [G loss: 0.822] [G acc: 0.125]\n",
      "291 [D loss: (0.660)(R 0.638, F 0.683)] [D acc: (0.594)(0.578, 0.609)] [G loss: 0.835] [G acc: 0.172]\n",
      "292 [D loss: (0.651)(R 0.626, F 0.675)] [D acc: (0.586)(0.594, 0.578)] [G loss: 0.909] [G acc: 0.172]\n",
      "293 [D loss: (0.694)(R 0.702, F 0.686)] [D acc: (0.531)(0.438, 0.625)] [G loss: 0.974] [G acc: 0.031]\n",
      "294 [D loss: (0.722)(R 0.768, F 0.676)] [D acc: (0.492)(0.328, 0.656)] [G loss: 0.844] [G acc: 0.188]\n",
      "295 [D loss: (0.681)(R 0.612, F 0.750)] [D acc: (0.578)(0.734, 0.422)] [G loss: 0.745] [G acc: 0.297]\n",
      "296 [D loss: (0.681)(R 0.617, F 0.746)] [D acc: (0.586)(0.656, 0.516)] [G loss: 0.788] [G acc: 0.281]\n",
      "297 [D loss: (0.631)(R 0.593, F 0.670)] [D acc: (0.633)(0.703, 0.562)] [G loss: 0.790] [G acc: 0.219]\n",
      "298 [D loss: (0.638)(R 0.617, F 0.660)] [D acc: (0.641)(0.703, 0.578)] [G loss: 0.951] [G acc: 0.078]\n",
      "299 [D loss: (0.644)(R 0.645, F 0.643)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.010] [G acc: 0.172]\n",
      "300 [D loss: (0.671)(R 0.582, F 0.759)] [D acc: (0.641)(0.719, 0.562)] [G loss: 0.957] [G acc: 0.109]\n",
      "301 [D loss: (0.670)(R 0.566, F 0.774)] [D acc: (0.570)(0.766, 0.375)] [G loss: 0.861] [G acc: 0.188]\n",
      "302 [D loss: (0.679)(R 0.655, F 0.703)] [D acc: (0.539)(0.562, 0.516)] [G loss: 0.830] [G acc: 0.203]\n",
      "303 [D loss: (0.667)(R 0.620, F 0.714)] [D acc: (0.539)(0.641, 0.438)] [G loss: 0.851] [G acc: 0.172]\n",
      "304 [D loss: (0.663)(R 0.630, F 0.695)] [D acc: (0.625)(0.641, 0.609)] [G loss: 0.839] [G acc: 0.156]\n",
      "305 [D loss: (0.660)(R 0.631, F 0.690)] [D acc: (0.586)(0.641, 0.531)] [G loss: 0.933] [G acc: 0.078]\n",
      "306 [D loss: (0.695)(R 0.658, F 0.731)] [D acc: (0.539)(0.578, 0.500)] [G loss: 0.820] [G acc: 0.094]\n",
      "307 [D loss: (0.652)(R 0.609, F 0.694)] [D acc: (0.664)(0.766, 0.562)] [G loss: 0.860] [G acc: 0.125]\n",
      "308 [D loss: (0.650)(R 0.618, F 0.682)] [D acc: (0.656)(0.688, 0.625)] [G loss: 0.956] [G acc: 0.031]\n",
      "309 [D loss: (0.671)(R 0.699, F 0.643)] [D acc: (0.609)(0.547, 0.672)] [G loss: 0.953] [G acc: 0.078]\n",
      "310 [D loss: (0.663)(R 0.656, F 0.670)] [D acc: (0.586)(0.594, 0.578)] [G loss: 0.822] [G acc: 0.188]\n",
      "311 [D loss: (0.652)(R 0.590, F 0.715)] [D acc: (0.602)(0.688, 0.516)] [G loss: 0.818] [G acc: 0.328]\n",
      "312 [D loss: (0.657)(R 0.592, F 0.723)] [D acc: (0.578)(0.672, 0.484)] [G loss: 0.821] [G acc: 0.250]\n",
      "313 [D loss: (0.666)(R 0.624, F 0.707)] [D acc: (0.609)(0.656, 0.562)] [G loss: 0.923] [G acc: 0.094]\n",
      "314 [D loss: (0.681)(R 0.635, F 0.727)] [D acc: (0.586)(0.656, 0.516)] [G loss: 0.874] [G acc: 0.141]\n",
      "315 [D loss: (0.658)(R 0.657, F 0.658)] [D acc: (0.586)(0.547, 0.625)] [G loss: 0.906] [G acc: 0.094]\n",
      "316 [D loss: (0.681)(R 0.652, F 0.710)] [D acc: (0.531)(0.609, 0.453)] [G loss: 0.840] [G acc: 0.188]\n",
      "317 [D loss: (0.648)(R 0.629, F 0.668)] [D acc: (0.672)(0.672, 0.672)] [G loss: 0.878] [G acc: 0.109]\n",
      "318 [D loss: (0.651)(R 0.639, F 0.664)] [D acc: (0.625)(0.594, 0.656)] [G loss: 0.875] [G acc: 0.109]\n",
      "319 [D loss: (0.636)(R 0.545, F 0.727)] [D acc: (0.617)(0.734, 0.500)] [G loss: 0.908] [G acc: 0.078]\n",
      "320 [D loss: (0.638)(R 0.550, F 0.727)] [D acc: (0.641)(0.766, 0.516)] [G loss: 0.961] [G acc: 0.078]\n",
      "321 [D loss: (0.614)(R 0.601, F 0.626)] [D acc: (0.719)(0.672, 0.766)] [G loss: 0.967] [G acc: 0.078]\n",
      "322 [D loss: (0.669)(R 0.631, F 0.707)] [D acc: (0.578)(0.578, 0.578)] [G loss: 0.908] [G acc: 0.094]\n",
      "323 [D loss: (0.639)(R 0.590, F 0.687)] [D acc: (0.594)(0.688, 0.500)] [G loss: 0.941] [G acc: 0.094]\n",
      "324 [D loss: (0.618)(R 0.565, F 0.672)] [D acc: (0.711)(0.797, 0.625)] [G loss: 1.042] [G acc: 0.062]\n",
      "325 [D loss: (0.690)(R 0.533, F 0.846)] [D acc: (0.570)(0.766, 0.375)] [G loss: 0.938] [G acc: 0.109]\n",
      "326 [D loss: (0.648)(R 0.604, F 0.693)] [D acc: (0.609)(0.688, 0.531)] [G loss: 0.885] [G acc: 0.188]\n",
      "327 [D loss: (0.659)(R 0.623, F 0.695)] [D acc: (0.594)(0.672, 0.516)] [G loss: 0.990] [G acc: 0.094]\n",
      "328 [D loss: (0.628)(R 0.643, F 0.613)] [D acc: (0.625)(0.562, 0.688)] [G loss: 0.880] [G acc: 0.219]\n",
      "329 [D loss: (0.668)(R 0.607, F 0.728)] [D acc: (0.625)(0.688, 0.562)] [G loss: 0.922] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330 [D loss: (0.652)(R 0.666, F 0.638)] [D acc: (0.648)(0.594, 0.703)] [G loss: 0.910] [G acc: 0.219]\n",
      "331 [D loss: (0.713)(R 0.611, F 0.815)] [D acc: (0.555)(0.703, 0.406)] [G loss: 0.915] [G acc: 0.172]\n",
      "332 [D loss: (0.640)(R 0.599, F 0.681)] [D acc: (0.625)(0.672, 0.578)] [G loss: 0.908] [G acc: 0.219]\n",
      "333 [D loss: (0.660)(R 0.651, F 0.669)] [D acc: (0.625)(0.656, 0.594)] [G loss: 0.879] [G acc: 0.203]\n",
      "334 [D loss: (0.680)(R 0.648, F 0.713)] [D acc: (0.516)(0.547, 0.484)] [G loss: 0.908] [G acc: 0.188]\n",
      "335 [D loss: (0.654)(R 0.642, F 0.667)] [D acc: (0.625)(0.641, 0.609)] [G loss: 0.881] [G acc: 0.109]\n",
      "336 [D loss: (0.630)(R 0.609, F 0.652)] [D acc: (0.672)(0.688, 0.656)] [G loss: 0.923] [G acc: 0.141]\n",
      "337 [D loss: (0.637)(R 0.596, F 0.678)] [D acc: (0.633)(0.656, 0.609)] [G loss: 0.921] [G acc: 0.141]\n",
      "338 [D loss: (0.650)(R 0.569, F 0.731)] [D acc: (0.602)(0.688, 0.516)] [G loss: 0.916] [G acc: 0.109]\n",
      "339 [D loss: (0.661)(R 0.592, F 0.729)] [D acc: (0.633)(0.688, 0.578)] [G loss: 0.909] [G acc: 0.094]\n",
      "340 [D loss: (0.664)(R 0.662, F 0.666)] [D acc: (0.594)(0.578, 0.609)] [G loss: 0.992] [G acc: 0.062]\n",
      "341 [D loss: (0.661)(R 0.670, F 0.653)] [D acc: (0.578)(0.469, 0.688)] [G loss: 0.914] [G acc: 0.125]\n",
      "342 [D loss: (0.691)(R 0.603, F 0.778)] [D acc: (0.586)(0.750, 0.422)] [G loss: 0.953] [G acc: 0.078]\n",
      "343 [D loss: (0.665)(R 0.655, F 0.675)] [D acc: (0.594)(0.594, 0.594)] [G loss: 0.894] [G acc: 0.125]\n",
      "344 [D loss: (0.694)(R 0.669, F 0.719)] [D acc: (0.555)(0.531, 0.578)] [G loss: 0.854] [G acc: 0.188]\n",
      "345 [D loss: (0.637)(R 0.592, F 0.682)] [D acc: (0.672)(0.672, 0.672)] [G loss: 0.886] [G acc: 0.156]\n",
      "346 [D loss: (0.648)(R 0.638, F 0.658)] [D acc: (0.617)(0.594, 0.641)] [G loss: 0.965] [G acc: 0.031]\n",
      "347 [D loss: (0.639)(R 0.634, F 0.645)] [D acc: (0.609)(0.562, 0.656)] [G loss: 0.936] [G acc: 0.156]\n",
      "348 [D loss: (0.677)(R 0.704, F 0.650)] [D acc: (0.594)(0.531, 0.656)] [G loss: 0.850] [G acc: 0.234]\n",
      "349 [D loss: (0.645)(R 0.558, F 0.733)] [D acc: (0.609)(0.719, 0.500)] [G loss: 0.930] [G acc: 0.109]\n",
      "350 [D loss: (0.610)(R 0.563, F 0.657)] [D acc: (0.672)(0.719, 0.625)] [G loss: 1.031] [G acc: 0.109]\n",
      "351 [D loss: (0.635)(R 0.653, F 0.616)] [D acc: (0.617)(0.578, 0.656)] [G loss: 0.961] [G acc: 0.141]\n",
      "352 [D loss: (0.622)(R 0.543, F 0.702)] [D acc: (0.648)(0.703, 0.594)] [G loss: 0.974] [G acc: 0.109]\n",
      "353 [D loss: (0.623)(R 0.580, F 0.667)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.008] [G acc: 0.125]\n",
      "354 [D loss: (0.597)(R 0.551, F 0.644)] [D acc: (0.688)(0.766, 0.609)] [G loss: 1.022] [G acc: 0.156]\n",
      "355 [D loss: (0.686)(R 0.653, F 0.719)] [D acc: (0.555)(0.578, 0.531)] [G loss: 0.902] [G acc: 0.062]\n",
      "356 [D loss: (0.652)(R 0.574, F 0.729)] [D acc: (0.656)(0.719, 0.594)] [G loss: 0.890] [G acc: 0.188]\n",
      "357 [D loss: (0.670)(R 0.672, F 0.668)] [D acc: (0.609)(0.547, 0.672)] [G loss: 0.859] [G acc: 0.188]\n",
      "358 [D loss: (0.664)(R 0.596, F 0.732)] [D acc: (0.617)(0.672, 0.562)] [G loss: 0.888] [G acc: 0.188]\n",
      "359 [D loss: (0.668)(R 0.627, F 0.710)] [D acc: (0.617)(0.672, 0.562)] [G loss: 0.835] [G acc: 0.172]\n",
      "360 [D loss: (0.663)(R 0.642, F 0.684)] [D acc: (0.547)(0.500, 0.594)] [G loss: 0.877] [G acc: 0.188]\n",
      "361 [D loss: (0.648)(R 0.608, F 0.687)] [D acc: (0.664)(0.672, 0.656)] [G loss: 0.866] [G acc: 0.109]\n",
      "362 [D loss: (0.675)(R 0.655, F 0.695)] [D acc: (0.578)(0.578, 0.578)] [G loss: 0.882] [G acc: 0.172]\n",
      "363 [D loss: (0.665)(R 0.637, F 0.692)] [D acc: (0.602)(0.656, 0.547)] [G loss: 0.871] [G acc: 0.219]\n",
      "364 [D loss: (0.635)(R 0.594, F 0.675)] [D acc: (0.648)(0.656, 0.641)] [G loss: 0.876] [G acc: 0.141]\n",
      "365 [D loss: (0.611)(R 0.566, F 0.657)] [D acc: (0.680)(0.734, 0.625)] [G loss: 0.865] [G acc: 0.250]\n",
      "366 [D loss: (0.651)(R 0.611, F 0.691)] [D acc: (0.609)(0.609, 0.609)] [G loss: 0.903] [G acc: 0.078]\n",
      "367 [D loss: (0.623)(R 0.539, F 0.707)] [D acc: (0.648)(0.688, 0.609)] [G loss: 0.996] [G acc: 0.188]\n",
      "368 [D loss: (0.635)(R 0.663, F 0.608)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.024] [G acc: 0.141]\n",
      "369 [D loss: (0.634)(R 0.539, F 0.729)] [D acc: (0.602)(0.656, 0.547)] [G loss: 0.998] [G acc: 0.172]\n",
      "370 [D loss: (0.655)(R 0.578, F 0.732)] [D acc: (0.656)(0.719, 0.594)] [G loss: 0.940] [G acc: 0.156]\n",
      "371 [D loss: (0.685)(R 0.588, F 0.783)] [D acc: (0.523)(0.672, 0.375)] [G loss: 0.929] [G acc: 0.109]\n",
      "372 [D loss: (0.635)(R 0.604, F 0.666)] [D acc: (0.633)(0.672, 0.594)] [G loss: 0.957] [G acc: 0.188]\n",
      "373 [D loss: (0.636)(R 0.608, F 0.665)] [D acc: (0.617)(0.656, 0.578)] [G loss: 0.957] [G acc: 0.203]\n",
      "374 [D loss: (0.614)(R 0.611, F 0.618)] [D acc: (0.711)(0.672, 0.750)] [G loss: 0.914] [G acc: 0.188]\n",
      "375 [D loss: (0.672)(R 0.567, F 0.776)] [D acc: (0.602)(0.719, 0.484)] [G loss: 1.003] [G acc: 0.094]\n",
      "376 [D loss: (0.638)(R 0.604, F 0.671)] [D acc: (0.664)(0.688, 0.641)] [G loss: 0.975] [G acc: 0.094]\n",
      "377 [D loss: (0.651)(R 0.592, F 0.710)] [D acc: (0.578)(0.703, 0.453)] [G loss: 0.932] [G acc: 0.172]\n",
      "378 [D loss: (0.669)(R 0.626, F 0.712)] [D acc: (0.547)(0.578, 0.516)] [G loss: 0.898] [G acc: 0.156]\n",
      "379 [D loss: (0.627)(R 0.588, F 0.665)] [D acc: (0.656)(0.734, 0.578)] [G loss: 0.932] [G acc: 0.188]\n",
      "380 [D loss: (0.673)(R 0.657, F 0.689)] [D acc: (0.547)(0.547, 0.547)] [G loss: 0.989] [G acc: 0.062]\n",
      "381 [D loss: (0.627)(R 0.636, F 0.619)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.010] [G acc: 0.094]\n",
      "382 [D loss: (0.661)(R 0.611, F 0.711)] [D acc: (0.680)(0.734, 0.625)] [G loss: 0.888] [G acc: 0.203]\n",
      "383 [D loss: (0.647)(R 0.560, F 0.734)] [D acc: (0.633)(0.719, 0.547)] [G loss: 1.109] [G acc: 0.094]\n",
      "384 [D loss: (0.665)(R 0.687, F 0.644)] [D acc: (0.648)(0.531, 0.766)] [G loss: 0.981] [G acc: 0.078]\n",
      "385 [D loss: (0.658)(R 0.620, F 0.695)] [D acc: (0.617)(0.625, 0.609)] [G loss: 0.929] [G acc: 0.156]\n",
      "386 [D loss: (0.586)(R 0.543, F 0.629)] [D acc: (0.750)(0.766, 0.734)] [G loss: 1.044] [G acc: 0.109]\n",
      "387 [D loss: (0.657)(R 0.644, F 0.670)] [D acc: (0.617)(0.609, 0.625)] [G loss: 0.974] [G acc: 0.125]\n",
      "388 [D loss: (0.617)(R 0.620, F 0.615)] [D acc: (0.695)(0.688, 0.703)] [G loss: 0.953] [G acc: 0.125]\n",
      "389 [D loss: (0.637)(R 0.611, F 0.663)] [D acc: (0.594)(0.578, 0.609)] [G loss: 0.900] [G acc: 0.188]\n",
      "390 [D loss: (0.644)(R 0.600, F 0.687)] [D acc: (0.617)(0.656, 0.578)] [G loss: 0.978] [G acc: 0.141]\n",
      "391 [D loss: (0.616)(R 0.616, F 0.616)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.018] [G acc: 0.109]\n",
      "392 [D loss: (0.607)(R 0.565, F 0.649)] [D acc: (0.641)(0.672, 0.609)] [G loss: 1.335] [G acc: 0.062]\n",
      "393 [D loss: (0.626)(R 0.618, F 0.634)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.073] [G acc: 0.062]\n",
      "394 [D loss: (0.611)(R 0.555, F 0.667)] [D acc: (0.672)(0.734, 0.609)] [G loss: 1.018] [G acc: 0.125]\n",
      "395 [D loss: (0.696)(R 0.514, F 0.878)] [D acc: (0.625)(0.766, 0.484)] [G loss: 0.980] [G acc: 0.078]\n",
      "396 [D loss: (0.604)(R 0.638, F 0.570)] [D acc: (0.711)(0.625, 0.797)] [G loss: 0.958] [G acc: 0.188]\n",
      "397 [D loss: (0.607)(R 0.583, F 0.630)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.058] [G acc: 0.156]\n",
      "398 [D loss: (0.616)(R 0.580, F 0.652)] [D acc: (0.633)(0.625, 0.641)] [G loss: 0.900] [G acc: 0.188]\n",
      "399 [D loss: (0.643)(R 0.523, F 0.763)] [D acc: (0.641)(0.703, 0.578)] [G loss: 0.929] [G acc: 0.219]\n",
      "400 [D loss: (0.614)(R 0.608, F 0.621)] [D acc: (0.695)(0.703, 0.688)] [G loss: 0.975] [G acc: 0.172]\n",
      "401 [D loss: (0.639)(R 0.566, F 0.712)] [D acc: (0.633)(0.688, 0.578)] [G loss: 1.063] [G acc: 0.125]\n",
      "402 [D loss: (0.633)(R 0.643, F 0.623)] [D acc: (0.609)(0.562, 0.656)] [G loss: 0.952] [G acc: 0.141]\n",
      "403 [D loss: (0.659)(R 0.650, F 0.668)] [D acc: (0.609)(0.562, 0.656)] [G loss: 1.065] [G acc: 0.125]\n",
      "404 [D loss: (0.691)(R 0.591, F 0.790)] [D acc: (0.578)(0.672, 0.484)] [G loss: 0.961] [G acc: 0.125]\n",
      "405 [D loss: (0.640)(R 0.634, F 0.646)] [D acc: (0.641)(0.609, 0.672)] [G loss: 0.913] [G acc: 0.141]\n",
      "406 [D loss: (0.612)(R 0.613, F 0.610)] [D acc: (0.648)(0.594, 0.703)] [G loss: 0.975] [G acc: 0.125]\n",
      "407 [D loss: (0.610)(R 0.620, F 0.601)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.165] [G acc: 0.141]\n",
      "408 [D loss: (0.654)(R 0.696, F 0.612)] [D acc: (0.641)(0.516, 0.766)] [G loss: 0.972] [G acc: 0.125]\n",
      "409 [D loss: (0.661)(R 0.598, F 0.724)] [D acc: (0.609)(0.641, 0.578)] [G loss: 0.922] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410 [D loss: (0.636)(R 0.589, F 0.684)] [D acc: (0.680)(0.672, 0.688)] [G loss: 0.989] [G acc: 0.109]\n",
      "411 [D loss: (0.652)(R 0.564, F 0.740)] [D acc: (0.586)(0.672, 0.500)] [G loss: 0.904] [G acc: 0.250]\n",
      "412 [D loss: (0.606)(R 0.561, F 0.650)] [D acc: (0.695)(0.672, 0.719)] [G loss: 0.959] [G acc: 0.203]\n",
      "413 [D loss: (0.624)(R 0.554, F 0.695)] [D acc: (0.672)(0.719, 0.625)] [G loss: 0.975] [G acc: 0.188]\n",
      "414 [D loss: (0.634)(R 0.578, F 0.691)] [D acc: (0.664)(0.719, 0.609)] [G loss: 1.042] [G acc: 0.156]\n",
      "415 [D loss: (0.646)(R 0.660, F 0.633)] [D acc: (0.617)(0.562, 0.672)] [G loss: 0.990] [G acc: 0.203]\n",
      "416 [D loss: (0.692)(R 0.782, F 0.602)] [D acc: (0.562)(0.422, 0.703)] [G loss: 1.225] [G acc: 0.047]\n",
      "417 [D loss: (0.666)(R 0.752, F 0.580)] [D acc: (0.625)(0.406, 0.844)] [G loss: 0.979] [G acc: 0.141]\n",
      "418 [D loss: (0.645)(R 0.507, F 0.782)] [D acc: (0.633)(0.750, 0.516)] [G loss: 0.972] [G acc: 0.078]\n",
      "419 [D loss: (0.676)(R 0.629, F 0.723)] [D acc: (0.578)(0.578, 0.578)] [G loss: 0.944] [G acc: 0.141]\n",
      "420 [D loss: (0.618)(R 0.586, F 0.650)] [D acc: (0.688)(0.734, 0.641)] [G loss: 0.955] [G acc: 0.141]\n",
      "421 [D loss: (0.689)(R 0.698, F 0.681)] [D acc: (0.516)(0.484, 0.547)] [G loss: 0.983] [G acc: 0.125]\n",
      "422 [D loss: (0.641)(R 0.630, F 0.652)] [D acc: (0.648)(0.641, 0.656)] [G loss: 1.020] [G acc: 0.125]\n",
      "423 [D loss: (0.657)(R 0.619, F 0.695)] [D acc: (0.617)(0.656, 0.578)] [G loss: 1.037] [G acc: 0.094]\n",
      "424 [D loss: (0.600)(R 0.628, F 0.572)] [D acc: (0.727)(0.703, 0.750)] [G loss: 0.959] [G acc: 0.203]\n",
      "425 [D loss: (0.656)(R 0.646, F 0.667)] [D acc: (0.617)(0.578, 0.656)] [G loss: 1.049] [G acc: 0.203]\n",
      "426 [D loss: (0.633)(R 0.618, F 0.648)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.001] [G acc: 0.141]\n",
      "427 [D loss: (0.634)(R 0.571, F 0.696)] [D acc: (0.656)(0.719, 0.594)] [G loss: 1.105] [G acc: 0.078]\n",
      "428 [D loss: (0.670)(R 0.680, F 0.660)] [D acc: (0.539)(0.531, 0.547)] [G loss: 0.936] [G acc: 0.172]\n",
      "429 [D loss: (0.581)(R 0.546, F 0.617)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.053] [G acc: 0.125]\n",
      "430 [D loss: (0.578)(R 0.550, F 0.605)] [D acc: (0.688)(0.719, 0.656)] [G loss: 1.057] [G acc: 0.109]\n",
      "431 [D loss: (0.662)(R 0.679, F 0.645)] [D acc: (0.602)(0.562, 0.641)] [G loss: 1.012] [G acc: 0.203]\n",
      "432 [D loss: (0.594)(R 0.599, F 0.588)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.034] [G acc: 0.172]\n",
      "433 [D loss: (0.620)(R 0.505, F 0.734)] [D acc: (0.648)(0.719, 0.578)] [G loss: 1.142] [G acc: 0.078]\n",
      "434 [D loss: (0.656)(R 0.579, F 0.733)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.073] [G acc: 0.156]\n",
      "435 [D loss: (0.616)(R 0.554, F 0.679)] [D acc: (0.672)(0.703, 0.641)] [G loss: 1.110] [G acc: 0.156]\n",
      "436 [D loss: (0.647)(R 0.623, F 0.670)] [D acc: (0.609)(0.625, 0.594)] [G loss: 1.006] [G acc: 0.141]\n",
      "437 [D loss: (0.621)(R 0.582, F 0.660)] [D acc: (0.609)(0.656, 0.562)] [G loss: 1.046] [G acc: 0.250]\n",
      "438 [D loss: (0.579)(R 0.528, F 0.630)] [D acc: (0.750)(0.797, 0.703)] [G loss: 1.079] [G acc: 0.109]\n",
      "439 [D loss: (0.614)(R 0.645, F 0.583)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.099] [G acc: 0.172]\n",
      "440 [D loss: (0.656)(R 0.617, F 0.694)] [D acc: (0.594)(0.641, 0.547)] [G loss: 1.130] [G acc: 0.094]\n",
      "441 [D loss: (0.639)(R 0.592, F 0.686)] [D acc: (0.641)(0.656, 0.625)] [G loss: 1.025] [G acc: 0.172]\n",
      "442 [D loss: (0.656)(R 0.599, F 0.713)] [D acc: (0.609)(0.672, 0.547)] [G loss: 1.000] [G acc: 0.172]\n",
      "443 [D loss: (0.585)(R 0.589, F 0.582)] [D acc: (0.711)(0.672, 0.750)] [G loss: 0.960] [G acc: 0.250]\n",
      "444 [D loss: (0.642)(R 0.597, F 0.686)] [D acc: (0.609)(0.594, 0.625)] [G loss: 1.132] [G acc: 0.219]\n",
      "445 [D loss: (0.629)(R 0.618, F 0.639)] [D acc: (0.641)(0.688, 0.594)] [G loss: 1.061] [G acc: 0.172]\n",
      "446 [D loss: (0.626)(R 0.613, F 0.639)] [D acc: (0.664)(0.688, 0.641)] [G loss: 0.968] [G acc: 0.188]\n",
      "447 [D loss: (0.646)(R 0.606, F 0.686)] [D acc: (0.641)(0.656, 0.625)] [G loss: 0.960] [G acc: 0.250]\n",
      "448 [D loss: (0.639)(R 0.593, F 0.686)] [D acc: (0.641)(0.672, 0.609)] [G loss: 1.049] [G acc: 0.109]\n",
      "449 [D loss: (0.597)(R 0.577, F 0.617)] [D acc: (0.641)(0.672, 0.609)] [G loss: 0.971] [G acc: 0.266]\n",
      "450 [D loss: (0.619)(R 0.600, F 0.639)] [D acc: (0.648)(0.625, 0.672)] [G loss: 0.995] [G acc: 0.125]\n",
      "451 [D loss: (0.612)(R 0.599, F 0.625)] [D acc: (0.734)(0.734, 0.734)] [G loss: 0.980] [G acc: 0.172]\n",
      "452 [D loss: (0.688)(R 0.532, F 0.843)] [D acc: (0.578)(0.703, 0.453)] [G loss: 0.989] [G acc: 0.250]\n",
      "453 [D loss: (0.670)(R 0.660, F 0.681)] [D acc: (0.617)(0.578, 0.656)] [G loss: 1.114] [G acc: 0.141]\n",
      "454 [D loss: (0.609)(R 0.620, F 0.598)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.042] [G acc: 0.141]\n",
      "455 [D loss: (0.589)(R 0.598, F 0.580)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.029] [G acc: 0.109]\n",
      "456 [D loss: (0.636)(R 0.576, F 0.697)] [D acc: (0.570)(0.609, 0.531)] [G loss: 1.136] [G acc: 0.078]\n",
      "457 [D loss: (0.624)(R 0.650, F 0.598)] [D acc: (0.672)(0.625, 0.719)] [G loss: 0.982] [G acc: 0.219]\n",
      "458 [D loss: (0.644)(R 0.637, F 0.651)] [D acc: (0.633)(0.609, 0.656)] [G loss: 1.052] [G acc: 0.125]\n",
      "459 [D loss: (0.548)(R 0.564, F 0.533)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.062] [G acc: 0.172]\n",
      "460 [D loss: (0.639)(R 0.531, F 0.746)] [D acc: (0.648)(0.719, 0.578)] [G loss: 1.132] [G acc: 0.188]\n",
      "461 [D loss: (0.625)(R 0.601, F 0.648)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.098] [G acc: 0.094]\n",
      "462 [D loss: (0.551)(R 0.538, F 0.564)] [D acc: (0.797)(0.750, 0.844)] [G loss: 1.116] [G acc: 0.156]\n",
      "463 [D loss: (0.790)(R 0.717, F 0.864)] [D acc: (0.477)(0.516, 0.438)] [G loss: 1.018] [G acc: 0.156]\n",
      "464 [D loss: (0.676)(R 0.720, F 0.632)] [D acc: (0.555)(0.516, 0.594)] [G loss: 0.962] [G acc: 0.172]\n",
      "465 [D loss: (0.637)(R 0.553, F 0.721)] [D acc: (0.641)(0.719, 0.562)] [G loss: 1.043] [G acc: 0.125]\n",
      "466 [D loss: (0.661)(R 0.654, F 0.667)] [D acc: (0.578)(0.547, 0.609)] [G loss: 0.950] [G acc: 0.156]\n",
      "467 [D loss: (0.648)(R 0.654, F 0.643)] [D acc: (0.664)(0.594, 0.734)] [G loss: 0.944] [G acc: 0.156]\n",
      "468 [D loss: (0.666)(R 0.635, F 0.697)] [D acc: (0.625)(0.641, 0.609)] [G loss: 0.938] [G acc: 0.172]\n",
      "469 [D loss: (0.647)(R 0.647, F 0.646)] [D acc: (0.633)(0.656, 0.609)] [G loss: 0.905] [G acc: 0.172]\n",
      "470 [D loss: (0.664)(R 0.636, F 0.693)] [D acc: (0.602)(0.625, 0.578)] [G loss: 0.930] [G acc: 0.188]\n",
      "471 [D loss: (0.633)(R 0.648, F 0.618)] [D acc: (0.609)(0.562, 0.656)] [G loss: 0.953] [G acc: 0.172]\n",
      "472 [D loss: (0.631)(R 0.630, F 0.632)] [D acc: (0.664)(0.672, 0.656)] [G loss: 1.016] [G acc: 0.125]\n",
      "473 [D loss: (0.622)(R 0.641, F 0.602)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.020] [G acc: 0.094]\n",
      "474 [D loss: (0.631)(R 0.633, F 0.629)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.088] [G acc: 0.156]\n",
      "475 [D loss: (0.664)(R 0.641, F 0.687)] [D acc: (0.641)(0.578, 0.703)] [G loss: 0.999] [G acc: 0.156]\n",
      "476 [D loss: (0.640)(R 0.641, F 0.640)] [D acc: (0.617)(0.609, 0.625)] [G loss: 1.000] [G acc: 0.156]\n",
      "477 [D loss: (0.633)(R 0.618, F 0.648)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.018] [G acc: 0.094]\n",
      "478 [D loss: (0.659)(R 0.640, F 0.677)] [D acc: (0.602)(0.547, 0.656)] [G loss: 0.986] [G acc: 0.172]\n",
      "479 [D loss: (0.648)(R 0.624, F 0.671)] [D acc: (0.617)(0.641, 0.594)] [G loss: 1.039] [G acc: 0.109]\n",
      "480 [D loss: (0.646)(R 0.651, F 0.641)] [D acc: (0.625)(0.594, 0.656)] [G loss: 0.989] [G acc: 0.172]\n",
      "481 [D loss: (0.650)(R 0.668, F 0.632)] [D acc: (0.633)(0.531, 0.734)] [G loss: 0.921] [G acc: 0.188]\n",
      "482 [D loss: (0.663)(R 0.598, F 0.728)] [D acc: (0.547)(0.578, 0.516)] [G loss: 0.916] [G acc: 0.188]\n",
      "483 [D loss: (0.580)(R 0.531, F 0.630)] [D acc: (0.742)(0.781, 0.703)] [G loss: 1.007] [G acc: 0.125]\n",
      "484 [D loss: (0.649)(R 0.630, F 0.669)] [D acc: (0.625)(0.594, 0.656)] [G loss: 0.946] [G acc: 0.188]\n",
      "485 [D loss: (0.603)(R 0.549, F 0.657)] [D acc: (0.664)(0.719, 0.609)] [G loss: 0.974] [G acc: 0.188]\n",
      "486 [D loss: (0.631)(R 0.627, F 0.635)] [D acc: (0.625)(0.625, 0.625)] [G loss: 0.953] [G acc: 0.141]\n",
      "487 [D loss: (0.665)(R 0.566, F 0.764)] [D acc: (0.594)(0.672, 0.516)] [G loss: 0.936] [G acc: 0.250]\n",
      "488 [D loss: (0.661)(R 0.647, F 0.676)] [D acc: (0.570)(0.516, 0.625)] [G loss: 0.959] [G acc: 0.125]\n",
      "489 [D loss: (0.677)(R 0.627, F 0.727)] [D acc: (0.570)(0.547, 0.594)] [G loss: 0.974] [G acc: 0.078]\n",
      "490 [D loss: (0.618)(R 0.637, F 0.599)] [D acc: (0.633)(0.516, 0.750)] [G loss: 0.940] [G acc: 0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491 [D loss: (0.606)(R 0.591, F 0.622)] [D acc: (0.695)(0.703, 0.688)] [G loss: 0.919] [G acc: 0.172]\n",
      "492 [D loss: (0.659)(R 0.607, F 0.711)] [D acc: (0.586)(0.641, 0.531)] [G loss: 1.025] [G acc: 0.125]\n",
      "493 [D loss: (0.679)(R 0.643, F 0.714)] [D acc: (0.633)(0.656, 0.609)] [G loss: 0.885] [G acc: 0.219]\n",
      "494 [D loss: (0.652)(R 0.618, F 0.686)] [D acc: (0.602)(0.594, 0.609)] [G loss: 0.925] [G acc: 0.188]\n",
      "495 [D loss: (0.624)(R 0.586, F 0.661)] [D acc: (0.641)(0.641, 0.641)] [G loss: 0.974] [G acc: 0.109]\n",
      "496 [D loss: (0.646)(R 0.647, F 0.645)] [D acc: (0.656)(0.609, 0.703)] [G loss: 0.974] [G acc: 0.188]\n",
      "497 [D loss: (0.683)(R 0.715, F 0.651)] [D acc: (0.641)(0.547, 0.734)] [G loss: 0.941] [G acc: 0.094]\n",
      "498 [D loss: (0.627)(R 0.552, F 0.701)] [D acc: (0.672)(0.750, 0.594)] [G loss: 0.964] [G acc: 0.156]\n",
      "499 [D loss: (0.622)(R 0.581, F 0.663)] [D acc: (0.609)(0.578, 0.641)] [G loss: 0.982] [G acc: 0.094]\n",
      "500 [D loss: (0.639)(R 0.593, F 0.685)] [D acc: (0.672)(0.672, 0.672)] [G loss: 0.963] [G acc: 0.141]\n",
      "501 [D loss: (0.653)(R 0.669, F 0.638)] [D acc: (0.602)(0.484, 0.719)] [G loss: 0.944] [G acc: 0.219]\n",
      "502 [D loss: (0.697)(R 0.666, F 0.728)] [D acc: (0.586)(0.562, 0.609)] [G loss: 0.949] [G acc: 0.125]\n",
      "503 [D loss: (0.639)(R 0.591, F 0.688)] [D acc: (0.641)(0.656, 0.625)] [G loss: 0.942] [G acc: 0.172]\n",
      "504 [D loss: (0.624)(R 0.634, F 0.614)] [D acc: (0.648)(0.594, 0.703)] [G loss: 0.943] [G acc: 0.141]\n",
      "505 [D loss: (0.624)(R 0.586, F 0.662)] [D acc: (0.672)(0.688, 0.656)] [G loss: 1.128] [G acc: 0.109]\n",
      "506 [D loss: (0.617)(R 0.635, F 0.599)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.063] [G acc: 0.078]\n",
      "507 [D loss: (0.610)(R 0.556, F 0.664)] [D acc: (0.695)(0.688, 0.703)] [G loss: 0.935] [G acc: 0.266]\n",
      "508 [D loss: (0.616)(R 0.529, F 0.702)] [D acc: (0.664)(0.734, 0.594)] [G loss: 1.023] [G acc: 0.219]\n",
      "509 [D loss: (0.618)(R 0.599, F 0.637)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.055] [G acc: 0.125]\n",
      "510 [D loss: (0.649)(R 0.669, F 0.629)] [D acc: (0.641)(0.562, 0.719)] [G loss: 0.971] [G acc: 0.141]\n",
      "511 [D loss: (0.681)(R 0.612, F 0.750)] [D acc: (0.609)(0.703, 0.516)] [G loss: 1.053] [G acc: 0.109]\n",
      "512 [D loss: (0.602)(R 0.558, F 0.645)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.006] [G acc: 0.156]\n",
      "513 [D loss: (0.630)(R 0.561, F 0.699)] [D acc: (0.633)(0.688, 0.578)] [G loss: 1.162] [G acc: 0.047]\n",
      "514 [D loss: (0.606)(R 0.649, F 0.564)] [D acc: (0.711)(0.578, 0.844)] [G loss: 0.967] [G acc: 0.125]\n",
      "515 [D loss: (0.598)(R 0.517, F 0.679)] [D acc: (0.703)(0.766, 0.641)] [G loss: 1.051] [G acc: 0.109]\n",
      "516 [D loss: (0.580)(R 0.508, F 0.651)] [D acc: (0.727)(0.750, 0.703)] [G loss: 1.129] [G acc: 0.078]\n",
      "517 [D loss: (0.561)(R 0.535, F 0.586)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.067] [G acc: 0.188]\n",
      "518 [D loss: (0.690)(R 0.612, F 0.768)] [D acc: (0.531)(0.516, 0.547)] [G loss: 1.087] [G acc: 0.188]\n",
      "519 [D loss: (0.621)(R 0.649, F 0.594)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.029] [G acc: 0.141]\n",
      "520 [D loss: (0.627)(R 0.592, F 0.661)] [D acc: (0.578)(0.562, 0.594)] [G loss: 1.010] [G acc: 0.156]\n",
      "521 [D loss: (0.639)(R 0.594, F 0.685)] [D acc: (0.609)(0.656, 0.562)] [G loss: 1.121] [G acc: 0.062]\n",
      "522 [D loss: (0.590)(R 0.535, F 0.644)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.065] [G acc: 0.188]\n",
      "523 [D loss: (0.615)(R 0.652, F 0.578)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.049] [G acc: 0.188]\n",
      "524 [D loss: (0.672)(R 0.640, F 0.703)] [D acc: (0.594)(0.609, 0.578)] [G loss: 1.005] [G acc: 0.109]\n",
      "525 [D loss: (0.646)(R 0.547, F 0.745)] [D acc: (0.641)(0.703, 0.578)] [G loss: 0.976] [G acc: 0.219]\n",
      "526 [D loss: (0.651)(R 0.631, F 0.672)] [D acc: (0.648)(0.656, 0.641)] [G loss: 0.991] [G acc: 0.188]\n",
      "527 [D loss: (0.624)(R 0.554, F 0.694)] [D acc: (0.641)(0.672, 0.609)] [G loss: 1.114] [G acc: 0.109]\n",
      "528 [D loss: (0.661)(R 0.623, F 0.699)] [D acc: (0.641)(0.578, 0.703)] [G loss: 1.060] [G acc: 0.156]\n",
      "529 [D loss: (0.626)(R 0.660, F 0.592)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.123] [G acc: 0.109]\n",
      "530 [D loss: (0.671)(R 0.692, F 0.649)] [D acc: (0.578)(0.500, 0.656)] [G loss: 1.018] [G acc: 0.172]\n",
      "531 [D loss: (0.666)(R 0.662, F 0.669)] [D acc: (0.594)(0.625, 0.562)] [G loss: 0.978] [G acc: 0.125]\n",
      "532 [D loss: (0.637)(R 0.610, F 0.664)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.096] [G acc: 0.094]\n",
      "533 [D loss: (0.595)(R 0.570, F 0.620)] [D acc: (0.641)(0.672, 0.609)] [G loss: 1.016] [G acc: 0.203]\n",
      "534 [D loss: (0.649)(R 0.674, F 0.623)] [D acc: (0.633)(0.578, 0.688)] [G loss: 0.989] [G acc: 0.156]\n",
      "535 [D loss: (0.590)(R 0.570, F 0.610)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.067] [G acc: 0.062]\n",
      "536 [D loss: (0.593)(R 0.545, F 0.640)] [D acc: (0.680)(0.734, 0.625)] [G loss: 1.020] [G acc: 0.188]\n",
      "537 [D loss: (0.638)(R 0.648, F 0.629)] [D acc: (0.633)(0.594, 0.672)] [G loss: 1.140] [G acc: 0.094]\n",
      "538 [D loss: (0.672)(R 0.658, F 0.686)] [D acc: (0.609)(0.594, 0.625)] [G loss: 0.996] [G acc: 0.125]\n",
      "539 [D loss: (0.585)(R 0.594, F 0.575)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.012] [G acc: 0.141]\n",
      "540 [D loss: (0.577)(R 0.506, F 0.647)] [D acc: (0.695)(0.734, 0.656)] [G loss: 1.133] [G acc: 0.125]\n",
      "541 [D loss: (0.628)(R 0.601, F 0.655)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.067] [G acc: 0.125]\n",
      "542 [D loss: (0.633)(R 0.575, F 0.690)] [D acc: (0.633)(0.625, 0.641)] [G loss: 1.003] [G acc: 0.250]\n",
      "543 [D loss: (0.664)(R 0.587, F 0.740)] [D acc: (0.648)(0.656, 0.641)] [G loss: 1.094] [G acc: 0.094]\n",
      "544 [D loss: (0.663)(R 0.710, F 0.616)] [D acc: (0.602)(0.531, 0.672)] [G loss: 1.025] [G acc: 0.188]\n",
      "545 [D loss: (0.657)(R 0.620, F 0.694)] [D acc: (0.586)(0.562, 0.609)] [G loss: 0.931] [G acc: 0.141]\n",
      "546 [D loss: (0.616)(R 0.627, F 0.605)] [D acc: (0.711)(0.656, 0.766)] [G loss: 0.999] [G acc: 0.125]\n",
      "547 [D loss: (0.597)(R 0.620, F 0.573)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.010] [G acc: 0.125]\n",
      "548 [D loss: (0.593)(R 0.584, F 0.602)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.039] [G acc: 0.172]\n",
      "549 [D loss: (0.663)(R 0.570, F 0.756)] [D acc: (0.625)(0.688, 0.562)] [G loss: 0.985] [G acc: 0.172]\n",
      "550 [D loss: (0.684)(R 0.692, F 0.677)] [D acc: (0.609)(0.562, 0.656)] [G loss: 1.025] [G acc: 0.062]\n",
      "551 [D loss: (0.601)(R 0.598, F 0.605)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.082] [G acc: 0.125]\n",
      "552 [D loss: (0.556)(R 0.559, F 0.553)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.100] [G acc: 0.172]\n",
      "553 [D loss: (0.676)(R 0.637, F 0.714)] [D acc: (0.594)(0.656, 0.531)] [G loss: 1.042] [G acc: 0.188]\n",
      "554 [D loss: (0.593)(R 0.576, F 0.609)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.006] [G acc: 0.203]\n",
      "555 [D loss: (0.678)(R 0.533, F 0.824)] [D acc: (0.617)(0.688, 0.547)] [G loss: 1.043] [G acc: 0.172]\n",
      "556 [D loss: (0.605)(R 0.555, F 0.654)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.112] [G acc: 0.078]\n",
      "557 [D loss: (0.697)(R 0.651, F 0.743)] [D acc: (0.531)(0.531, 0.531)] [G loss: 1.041] [G acc: 0.172]\n",
      "558 [D loss: (0.580)(R 0.599, F 0.561)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.079] [G acc: 0.094]\n",
      "559 [D loss: (0.613)(R 0.536, F 0.690)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.247] [G acc: 0.125]\n",
      "560 [D loss: (0.682)(R 0.685, F 0.680)] [D acc: (0.578)(0.547, 0.609)] [G loss: 0.949] [G acc: 0.156]\n",
      "561 [D loss: (0.622)(R 0.590, F 0.655)] [D acc: (0.641)(0.609, 0.672)] [G loss: 0.991] [G acc: 0.172]\n",
      "562 [D loss: (0.577)(R 0.500, F 0.654)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.038] [G acc: 0.141]\n",
      "563 [D loss: (0.613)(R 0.607, F 0.619)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.028] [G acc: 0.172]\n",
      "564 [D loss: (0.621)(R 0.610, F 0.632)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.068] [G acc: 0.172]\n",
      "565 [D loss: (0.630)(R 0.576, F 0.684)] [D acc: (0.664)(0.703, 0.625)] [G loss: 1.063] [G acc: 0.156]\n",
      "566 [D loss: (0.622)(R 0.673, F 0.572)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.255] [G acc: 0.094]\n",
      "567 [D loss: (0.607)(R 0.572, F 0.643)] [D acc: (0.680)(0.719, 0.641)] [G loss: 1.032] [G acc: 0.156]\n",
      "568 [D loss: (0.598)(R 0.571, F 0.624)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.043] [G acc: 0.109]\n",
      "569 [D loss: (0.604)(R 0.640, F 0.567)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.072] [G acc: 0.141]\n",
      "570 [D loss: (0.560)(R 0.472, F 0.649)] [D acc: (0.773)(0.828, 0.719)] [G loss: 1.103] [G acc: 0.156]\n",
      "571 [D loss: (0.532)(R 0.458, F 0.607)] [D acc: (0.742)(0.766, 0.719)] [G loss: 1.269] [G acc: 0.062]\n",
      "572 [D loss: (0.637)(R 0.590, F 0.684)] [D acc: (0.664)(0.688, 0.641)] [G loss: 1.154] [G acc: 0.141]\n",
      "573 [D loss: (0.589)(R 0.585, F 0.592)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.227] [G acc: 0.078]\n",
      "574 [D loss: (0.657)(R 0.608, F 0.707)] [D acc: (0.602)(0.609, 0.594)] [G loss: 1.111] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575 [D loss: (0.601)(R 0.569, F 0.634)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.114] [G acc: 0.141]\n",
      "576 [D loss: (0.641)(R 0.586, F 0.695)] [D acc: (0.594)(0.641, 0.547)] [G loss: 1.164] [G acc: 0.094]\n",
      "577 [D loss: (0.583)(R 0.616, F 0.550)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.110] [G acc: 0.188]\n",
      "578 [D loss: (0.659)(R 0.557, F 0.761)] [D acc: (0.594)(0.656, 0.531)] [G loss: 1.121] [G acc: 0.156]\n",
      "579 [D loss: (0.628)(R 0.596, F 0.661)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.082] [G acc: 0.125]\n",
      "580 [D loss: (0.655)(R 0.638, F 0.673)] [D acc: (0.570)(0.562, 0.578)] [G loss: 1.008] [G acc: 0.156]\n",
      "581 [D loss: (0.568)(R 0.573, F 0.564)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.211] [G acc: 0.156]\n",
      "582 [D loss: (0.615)(R 0.617, F 0.613)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.065] [G acc: 0.141]\n",
      "583 [D loss: (0.579)(R 0.612, F 0.546)] [D acc: (0.680)(0.594, 0.766)] [G loss: 0.987] [G acc: 0.234]\n",
      "584 [D loss: (0.600)(R 0.534, F 0.665)] [D acc: (0.656)(0.703, 0.609)] [G loss: 1.019] [G acc: 0.219]\n",
      "585 [D loss: (0.585)(R 0.601, F 0.569)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.083] [G acc: 0.250]\n",
      "586 [D loss: (0.612)(R 0.629, F 0.594)] [D acc: (0.656)(0.625, 0.688)] [G loss: 0.962] [G acc: 0.141]\n",
      "587 [D loss: (0.584)(R 0.418, F 0.750)] [D acc: (0.656)(0.797, 0.516)] [G loss: 1.006] [G acc: 0.234]\n",
      "588 [D loss: (0.615)(R 0.574, F 0.656)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.039] [G acc: 0.188]\n",
      "589 [D loss: (0.657)(R 0.520, F 0.793)] [D acc: (0.617)(0.719, 0.516)] [G loss: 1.066] [G acc: 0.172]\n",
      "590 [D loss: (0.642)(R 0.630, F 0.654)] [D acc: (0.594)(0.516, 0.672)] [G loss: 1.032] [G acc: 0.172]\n",
      "591 [D loss: (0.639)(R 0.575, F 0.702)] [D acc: (0.617)(0.656, 0.578)] [G loss: 1.007] [G acc: 0.172]\n",
      "592 [D loss: (0.653)(R 0.649, F 0.656)] [D acc: (0.641)(0.641, 0.641)] [G loss: 0.928] [G acc: 0.141]\n",
      "593 [D loss: (0.585)(R 0.598, F 0.572)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.040] [G acc: 0.156]\n",
      "594 [D loss: (0.617)(R 0.620, F 0.613)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.039] [G acc: 0.078]\n",
      "595 [D loss: (0.619)(R 0.606, F 0.631)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.053] [G acc: 0.188]\n",
      "596 [D loss: (0.564)(R 0.562, F 0.566)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.082] [G acc: 0.125]\n",
      "597 [D loss: (0.613)(R 0.552, F 0.674)] [D acc: (0.656)(0.719, 0.594)] [G loss: 1.018] [G acc: 0.156]\n",
      "598 [D loss: (0.675)(R 0.610, F 0.741)] [D acc: (0.586)(0.594, 0.578)] [G loss: 1.063] [G acc: 0.141]\n",
      "599 [D loss: (0.622)(R 0.587, F 0.656)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.052] [G acc: 0.203]\n",
      "600 [D loss: (0.592)(R 0.603, F 0.580)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.035] [G acc: 0.172]\n",
      "601 [D loss: (0.641)(R 0.609, F 0.674)] [D acc: (0.625)(0.609, 0.641)] [G loss: 1.003] [G acc: 0.188]\n",
      "602 [D loss: (0.643)(R 0.628, F 0.658)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.091] [G acc: 0.172]\n",
      "603 [D loss: (0.613)(R 0.585, F 0.640)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.075] [G acc: 0.156]\n",
      "604 [D loss: (0.593)(R 0.555, F 0.632)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.155] [G acc: 0.094]\n",
      "605 [D loss: (0.620)(R 0.576, F 0.664)] [D acc: (0.664)(0.672, 0.656)] [G loss: 1.122] [G acc: 0.094]\n",
      "606 [D loss: (0.590)(R 0.568, F 0.612)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.137] [G acc: 0.094]\n",
      "607 [D loss: (0.624)(R 0.625, F 0.623)] [D acc: (0.633)(0.547, 0.719)] [G loss: 1.050] [G acc: 0.125]\n",
      "608 [D loss: (0.618)(R 0.565, F 0.671)] [D acc: (0.672)(0.703, 0.641)] [G loss: 1.170] [G acc: 0.141]\n",
      "609 [D loss: (0.683)(R 0.766, F 0.600)] [D acc: (0.617)(0.531, 0.703)] [G loss: 1.066] [G acc: 0.078]\n",
      "610 [D loss: (0.616)(R 0.601, F 0.630)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.029] [G acc: 0.141]\n",
      "611 [D loss: (0.688)(R 0.666, F 0.711)] [D acc: (0.570)(0.500, 0.641)] [G loss: 1.041] [G acc: 0.125]\n",
      "612 [D loss: (0.530)(R 0.501, F 0.559)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.106] [G acc: 0.188]\n",
      "613 [D loss: (0.623)(R 0.619, F 0.626)] [D acc: (0.633)(0.625, 0.641)] [G loss: 1.049] [G acc: 0.125]\n",
      "614 [D loss: (0.701)(R 0.649, F 0.753)] [D acc: (0.531)(0.516, 0.547)] [G loss: 0.979] [G acc: 0.250]\n",
      "615 [D loss: (0.605)(R 0.574, F 0.635)] [D acc: (0.656)(0.672, 0.641)] [G loss: 0.987] [G acc: 0.125]\n",
      "616 [D loss: (0.723)(R 0.648, F 0.798)] [D acc: (0.617)(0.625, 0.609)] [G loss: 1.013] [G acc: 0.156]\n",
      "617 [D loss: (0.605)(R 0.620, F 0.589)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.093] [G acc: 0.109]\n",
      "618 [D loss: (0.610)(R 0.593, F 0.626)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.172] [G acc: 0.109]\n",
      "619 [D loss: (0.571)(R 0.540, F 0.601)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.112] [G acc: 0.078]\n",
      "620 [D loss: (0.564)(R 0.552, F 0.577)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.113] [G acc: 0.078]\n",
      "621 [D loss: (0.708)(R 0.714, F 0.701)] [D acc: (0.555)(0.516, 0.594)] [G loss: 1.078] [G acc: 0.078]\n",
      "622 [D loss: (0.613)(R 0.611, F 0.614)] [D acc: (0.648)(0.562, 0.734)] [G loss: 0.979] [G acc: 0.141]\n",
      "623 [D loss: (0.585)(R 0.607, F 0.563)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.086] [G acc: 0.203]\n",
      "624 [D loss: (0.613)(R 0.556, F 0.669)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.043] [G acc: 0.203]\n",
      "625 [D loss: (0.638)(R 0.619, F 0.657)] [D acc: (0.656)(0.672, 0.641)] [G loss: 1.093] [G acc: 0.109]\n",
      "626 [D loss: (0.624)(R 0.598, F 0.650)] [D acc: (0.609)(0.625, 0.594)] [G loss: 1.016] [G acc: 0.125]\n",
      "627 [D loss: (0.619)(R 0.662, F 0.575)] [D acc: (0.633)(0.531, 0.734)] [G loss: 1.054] [G acc: 0.109]\n",
      "628 [D loss: (0.642)(R 0.660, F 0.624)] [D acc: (0.656)(0.547, 0.766)] [G loss: 0.991] [G acc: 0.172]\n",
      "629 [D loss: (0.614)(R 0.538, F 0.689)] [D acc: (0.672)(0.750, 0.594)] [G loss: 1.070] [G acc: 0.156]\n",
      "630 [D loss: (0.552)(R 0.524, F 0.580)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.073] [G acc: 0.078]\n",
      "631 [D loss: (0.625)(R 0.635, F 0.616)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.189] [G acc: 0.141]\n",
      "632 [D loss: (0.590)(R 0.624, F 0.555)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.093] [G acc: 0.125]\n",
      "633 [D loss: (0.615)(R 0.585, F 0.646)] [D acc: (0.633)(0.594, 0.672)] [G loss: 1.167] [G acc: 0.047]\n",
      "634 [D loss: (0.657)(R 0.623, F 0.691)] [D acc: (0.594)(0.531, 0.656)] [G loss: 0.996] [G acc: 0.156]\n",
      "635 [D loss: (0.609)(R 0.528, F 0.690)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.225] [G acc: 0.078]\n",
      "636 [D loss: (0.631)(R 0.614, F 0.648)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.202] [G acc: 0.109]\n",
      "637 [D loss: (0.562)(R 0.582, F 0.542)] [D acc: (0.750)(0.609, 0.891)] [G loss: 1.162] [G acc: 0.156]\n",
      "638 [D loss: (0.653)(R 0.655, F 0.651)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.049] [G acc: 0.188]\n",
      "639 [D loss: (0.628)(R 0.621, F 0.634)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.108] [G acc: 0.188]\n",
      "640 [D loss: (0.566)(R 0.558, F 0.574)] [D acc: (0.742)(0.766, 0.719)] [G loss: 1.107] [G acc: 0.188]\n",
      "641 [D loss: (0.651)(R 0.518, F 0.785)] [D acc: (0.703)(0.750, 0.656)] [G loss: 1.245] [G acc: 0.125]\n",
      "642 [D loss: (0.662)(R 0.656, F 0.667)] [D acc: (0.570)(0.578, 0.562)] [G loss: 1.036] [G acc: 0.188]\n",
      "643 [D loss: (0.676)(R 0.665, F 0.687)] [D acc: (0.570)(0.531, 0.609)] [G loss: 0.972] [G acc: 0.203]\n",
      "644 [D loss: (0.603)(R 0.555, F 0.651)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.002] [G acc: 0.297]\n",
      "645 [D loss: (0.619)(R 0.609, F 0.630)] [D acc: (0.609)(0.609, 0.609)] [G loss: 1.223] [G acc: 0.188]\n",
      "646 [D loss: (0.655)(R 0.708, F 0.602)] [D acc: (0.617)(0.500, 0.734)] [G loss: 0.991] [G acc: 0.188]\n",
      "647 [D loss: (0.659)(R 0.658, F 0.660)] [D acc: (0.609)(0.594, 0.625)] [G loss: 1.163] [G acc: 0.188]\n",
      "648 [D loss: (0.627)(R 0.654, F 0.601)] [D acc: (0.633)(0.547, 0.719)] [G loss: 0.957] [G acc: 0.109]\n",
      "649 [D loss: (0.598)(R 0.554, F 0.641)] [D acc: (0.719)(0.672, 0.766)] [G loss: 0.998] [G acc: 0.188]\n",
      "650 [D loss: (0.657)(R 0.616, F 0.699)] [D acc: (0.586)(0.578, 0.594)] [G loss: 0.959] [G acc: 0.172]\n",
      "651 [D loss: (0.638)(R 0.595, F 0.680)] [D acc: (0.633)(0.609, 0.656)] [G loss: 1.063] [G acc: 0.141]\n",
      "652 [D loss: (0.644)(R 0.661, F 0.626)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.039] [G acc: 0.188]\n",
      "653 [D loss: (0.650)(R 0.624, F 0.676)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.090] [G acc: 0.125]\n",
      "654 [D loss: (0.650)(R 0.643, F 0.657)] [D acc: (0.609)(0.625, 0.594)] [G loss: 0.960] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655 [D loss: (0.598)(R 0.607, F 0.590)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.009] [G acc: 0.156]\n",
      "656 [D loss: (0.626)(R 0.565, F 0.687)] [D acc: (0.648)(0.641, 0.656)] [G loss: 1.154] [G acc: 0.047]\n",
      "657 [D loss: (0.641)(R 0.655, F 0.627)] [D acc: (0.664)(0.609, 0.719)] [G loss: 0.978] [G acc: 0.141]\n",
      "658 [D loss: (0.622)(R 0.667, F 0.577)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.003] [G acc: 0.109]\n",
      "659 [D loss: (0.592)(R 0.592, F 0.593)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.019] [G acc: 0.141]\n",
      "660 [D loss: (0.624)(R 0.518, F 0.730)] [D acc: (0.641)(0.719, 0.562)] [G loss: 1.368] [G acc: 0.062]\n",
      "661 [D loss: (0.606)(R 0.675, F 0.537)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.089] [G acc: 0.125]\n",
      "662 [D loss: (0.650)(R 0.531, F 0.769)] [D acc: (0.672)(0.734, 0.609)] [G loss: 1.043] [G acc: 0.188]\n",
      "663 [D loss: (0.522)(R 0.487, F 0.556)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.172] [G acc: 0.141]\n",
      "664 [D loss: (0.724)(R 0.750, F 0.698)] [D acc: (0.555)(0.500, 0.609)] [G loss: 1.012] [G acc: 0.156]\n",
      "665 [D loss: (0.645)(R 0.567, F 0.723)] [D acc: (0.609)(0.672, 0.547)] [G loss: 1.020] [G acc: 0.156]\n",
      "666 [D loss: (0.575)(R 0.576, F 0.575)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.090] [G acc: 0.172]\n",
      "667 [D loss: (0.639)(R 0.630, F 0.648)] [D acc: (0.641)(0.641, 0.641)] [G loss: 1.086] [G acc: 0.078]\n",
      "668 [D loss: (0.544)(R 0.567, F 0.520)] [D acc: (0.734)(0.641, 0.828)] [G loss: 0.980] [G acc: 0.234]\n",
      "669 [D loss: (0.661)(R 0.610, F 0.712)] [D acc: (0.594)(0.609, 0.578)] [G loss: 1.040] [G acc: 0.172]\n",
      "670 [D loss: (0.560)(R 0.534, F 0.586)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.039] [G acc: 0.219]\n",
      "671 [D loss: (0.711)(R 0.577, F 0.845)] [D acc: (0.641)(0.719, 0.562)] [G loss: 1.204] [G acc: 0.125]\n",
      "672 [D loss: (0.542)(R 0.548, F 0.535)] [D acc: (0.789)(0.750, 0.828)] [G loss: 1.111] [G acc: 0.156]\n",
      "673 [D loss: (0.594)(R 0.595, F 0.592)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.091] [G acc: 0.156]\n",
      "674 [D loss: (0.627)(R 0.569, F 0.685)] [D acc: (0.602)(0.688, 0.516)] [G loss: 1.163] [G acc: 0.109]\n",
      "675 [D loss: (0.615)(R 0.615, F 0.614)] [D acc: (0.609)(0.531, 0.688)] [G loss: 1.212] [G acc: 0.141]\n",
      "676 [D loss: (0.650)(R 0.710, F 0.591)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.078] [G acc: 0.172]\n",
      "677 [D loss: (0.657)(R 0.609, F 0.705)] [D acc: (0.617)(0.672, 0.562)] [G loss: 1.056] [G acc: 0.188]\n",
      "678 [D loss: (0.598)(R 0.640, F 0.555)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.135] [G acc: 0.125]\n",
      "679 [D loss: (0.610)(R 0.563, F 0.657)] [D acc: (0.648)(0.656, 0.641)] [G loss: 1.073] [G acc: 0.203]\n",
      "680 [D loss: (0.620)(R 0.601, F 0.640)] [D acc: (0.633)(0.656, 0.609)] [G loss: 1.025] [G acc: 0.188]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-cef2b0cd8bca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m,\u001b[0m \u001b[0mrun_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRUN_FOLDER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;33m,\u001b[0m \u001b[0mprint_every_n_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPRINT_EVERY_N_BATCHES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;32m~\\Documents\\KuenstlicheIntelligenz\\git\\GDL_code\\models\\GAN.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_train, batch_size, epochs, run_folder, print_every_n_batches, using_generator)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musing_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\KuenstlicheIntelligenz\\git\\GDL_code\\models\\GAN.py\u001b[0m in \u001b[0;36mtrain_discriminator\u001b[1;34m(self, x_train, batch_size, using_generator)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0md_loss_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_acc_real\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         \u001b[0md_loss_fake\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_acc_fake\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0md_loss_real\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[0md_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0md_acc_real\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0md_acc_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GDL\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GDL\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GDL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GDL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GDL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GDL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GDL\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gan.train(     \n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x[0] for x in gan.d_losses], color='black', linewidth=0.25)\n",
    "\n",
    "plt.plot([x[1] for x in gan.d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[2] for x in gan.d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[0] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 2000)\n",
    "plt.ylim(0, 2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x[3] for x in gan.d_losses], color='black', linewidth=0.25)\n",
    "plt.plot([x[4] for x in gan.d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[5] for x in gan.d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[1] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('accuracy', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 2000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1330804cfc8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "noise = np.random.normal(0, 1, (BATCH_SIZE, 100))\n",
    "gen_imgs = gan.generator.predict(noise)\n",
    "plt.imshow(gen_imgs[25,:,:,0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.9999546 ],\n",
       "        [-0.9999995 ],\n",
       "        [-1.        ],\n",
       "        [-0.99999994],\n",
       "        [-1.        ],\n",
       "        [-0.99999994],\n",
       "        [-0.9999995 ],\n",
       "        [-0.9999991 ],\n",
       "        [-0.9999823 ],\n",
       "        [-0.9999635 ],\n",
       "        [-0.9999569 ],\n",
       "        [-0.9999533 ],\n",
       "        [-0.99992406],\n",
       "        [-0.9997191 ],\n",
       "        [-0.99870497],\n",
       "        [-0.99851483],\n",
       "        [-0.9987713 ],\n",
       "        [-0.9975936 ],\n",
       "        [-0.99406886],\n",
       "        [-0.9785458 ],\n",
       "        [-0.9767582 ],\n",
       "        [-0.97117835],\n",
       "        [-0.97850025],\n",
       "        [-0.995894  ],\n",
       "        [-0.9977897 ],\n",
       "        [-0.9996959 ],\n",
       "        [-0.99988157],\n",
       "        [-0.99528575]],\n",
       "\n",
       "       [[-0.9999997 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.99999994],\n",
       "        [-0.9999996 ],\n",
       "        [-0.9999983 ],\n",
       "        [-0.99999946],\n",
       "        [-0.9999998 ],\n",
       "        [-0.9999997 ],\n",
       "        [-0.9999992 ],\n",
       "        [-0.99999   ],\n",
       "        [-0.99998724],\n",
       "        [-0.9999906 ],\n",
       "        [-0.99999267],\n",
       "        [-0.99999934],\n",
       "        [-0.9999999 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.9999747 ]],\n",
       "\n",
       "       [[-0.99999994],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ]],\n",
       "\n",
       "       [[-0.9999997 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ]],\n",
       "\n",
       "       [[-0.9999837 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ]],\n",
       "\n",
       "       [[-0.9976361 ],\n",
       "        [-0.99999434],\n",
       "        [-0.99999994],\n",
       "        [-0.9999993 ],\n",
       "        [-0.9999751 ],\n",
       "        [-0.9998377 ],\n",
       "        [-0.9999142 ],\n",
       "        [-0.99999154],\n",
       "        [-0.99999964],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.99999994],\n",
       "        [-0.99999994],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ]],\n",
       "\n",
       "       [[-0.9742852 ],\n",
       "        [-0.9979805 ],\n",
       "        [-0.99986345],\n",
       "        [-0.9992701 ],\n",
       "        [-0.89976054],\n",
       "        [-0.40290752],\n",
       "        [-0.6222354 ],\n",
       "        [-0.99174017],\n",
       "        [-0.9999724 ],\n",
       "        [-0.9999997 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.9999999 ],\n",
       "        [-0.9999984 ],\n",
       "        [-0.9999926 ],\n",
       "        [-0.99999875],\n",
       "        [-0.99999994],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ]],\n",
       "\n",
       "       [[-0.9533168 ],\n",
       "        [-0.66540587],\n",
       "        [ 0.38854325],\n",
       "        [-0.17159131],\n",
       "        [ 0.90803987],\n",
       "        [ 0.9792921 ],\n",
       "        [ 0.98939866],\n",
       "        [ 0.8619636 ],\n",
       "        [-0.99363965],\n",
       "        [-0.9999986 ],\n",
       "        [-0.99999994],\n",
       "        [-0.9999999 ],\n",
       "        [-0.99999845],\n",
       "        [-0.99993217],\n",
       "        [-0.9981694 ],\n",
       "        [-0.99890953],\n",
       "        [-0.99995697],\n",
       "        [-0.99999845],\n",
       "        [-0.9999998 ],\n",
       "        [-0.99999994],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.99999994]],\n",
       "\n",
       "       [[-0.9930352 ],\n",
       "        [-0.19156833],\n",
       "        [ 0.9950198 ],\n",
       "        [ 0.82456225],\n",
       "        [ 0.21154116],\n",
       "        [-0.22147001],\n",
       "        [ 0.7724355 ],\n",
       "        [ 0.9812912 ],\n",
       "        [-0.14767316],\n",
       "        [-0.99999064],\n",
       "        [-1.        ],\n",
       "        [-0.9999998 ],\n",
       "        [-0.99998015],\n",
       "        [-0.99473894],\n",
       "        [-0.63631666],\n",
       "        [-0.23346257],\n",
       "        [-0.7098093 ],\n",
       "        [-0.9831186 ],\n",
       "        [-0.99845344],\n",
       "        [-0.99951315],\n",
       "        [-0.999921  ],\n",
       "        [-0.99995553],\n",
       "        [-0.9999495 ],\n",
       "        [-0.99995345],\n",
       "        [-0.99997944],\n",
       "        [-0.99999976],\n",
       "        [-0.9999999 ],\n",
       "        [-0.9999985 ]],\n",
       "\n",
       "       [[-0.9990526 ],\n",
       "        [-0.80947423],\n",
       "        [ 0.98479444],\n",
       "        [ 0.9726606 ],\n",
       "        [ 0.21131304],\n",
       "        [-0.96528417],\n",
       "        [-0.8899928 ],\n",
       "        [ 0.80821496],\n",
       "        [ 0.9697449 ],\n",
       "        [-0.9923444 ],\n",
       "        [-0.99999976],\n",
       "        [-0.999999  ],\n",
       "        [-0.99944574],\n",
       "        [-0.8833606 ],\n",
       "        [-0.43696734],\n",
       "        [-0.50698817],\n",
       "        [ 0.11685681],\n",
       "        [ 0.29923978],\n",
       "        [ 0.27006057],\n",
       "        [ 0.19553824],\n",
       "        [ 0.10507526],\n",
       "        [-0.06001059],\n",
       "        [-0.20338875],\n",
       "        [-0.06127899],\n",
       "        [-0.5829963 ],\n",
       "        [-0.9974822 ],\n",
       "        [-0.99998134],\n",
       "        [-0.99994236]],\n",
       "\n",
       "       [[-0.99995613],\n",
       "        [-0.99885505],\n",
       "        [-0.8451714 ],\n",
       "        [ 0.98037064],\n",
       "        [ 0.9458641 ],\n",
       "        [-0.9026165 ],\n",
       "        [-0.9727742 ],\n",
       "        [-0.7937441 ],\n",
       "        [ 0.9551872 ],\n",
       "        [ 0.89048886],\n",
       "        [-0.9979355 ],\n",
       "        [-0.99823004],\n",
       "        [-0.54610324],\n",
       "        [ 0.11144774],\n",
       "        [-0.95934165],\n",
       "        [-0.99752045],\n",
       "        [-0.97754633],\n",
       "        [-0.6044253 ],\n",
       "        [ 0.23186117],\n",
       "        [ 0.4586074 ],\n",
       "        [ 0.7286391 ],\n",
       "        [ 0.74889004],\n",
       "        [ 0.63157976],\n",
       "        [ 0.8845742 ],\n",
       "        [ 0.8465617 ],\n",
       "        [ 0.05967087],\n",
       "        [-0.9977891 ],\n",
       "        [-0.99944144]],\n",
       "\n",
       "       [[-0.9999998 ],\n",
       "        [-0.9999999 ],\n",
       "        [-0.99999976],\n",
       "        [-0.98098654],\n",
       "        [-0.0052752 ],\n",
       "        [-0.98389065],\n",
       "        [-0.9977902 ],\n",
       "        [-0.9941262 ],\n",
       "        [-0.6893734 ],\n",
       "        [ 0.9925    ],\n",
       "        [ 0.8946829 ],\n",
       "        [ 0.70574045],\n",
       "        [ 0.86182535],\n",
       "        [-0.6723865 ],\n",
       "        [-0.9988    ],\n",
       "        [-0.99971384],\n",
       "        [-0.99622667],\n",
       "        [-0.9245495 ],\n",
       "        [-0.5818022 ],\n",
       "        [-0.49494925],\n",
       "        [-0.25141877],\n",
       "        [-0.06886096],\n",
       "        [ 0.12640929],\n",
       "        [ 0.750265  ],\n",
       "        [ 0.930437  ],\n",
       "        [ 0.99426115],\n",
       "        [-0.35979447],\n",
       "        [-0.9949475 ]],\n",
       "\n",
       "       [[-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.9999954 ],\n",
       "        [-0.8048606 ],\n",
       "        [-0.97037446],\n",
       "        [-0.9995282 ],\n",
       "        [-0.99462944],\n",
       "        [-0.965505  ],\n",
       "        [-0.23488177],\n",
       "        [ 0.7228991 ],\n",
       "        [ 0.16308925],\n",
       "        [-0.96310496],\n",
       "        [-0.9998089 ],\n",
       "        [-0.99998844],\n",
       "        [-0.9999739 ],\n",
       "        [-0.9991342 ],\n",
       "        [-0.9613612 ],\n",
       "        [-0.50133836],\n",
       "        [-0.07578053],\n",
       "        [ 0.00891843],\n",
       "        [ 0.4610683 ],\n",
       "        [ 0.7600722 ],\n",
       "        [ 0.8474028 ],\n",
       "        [ 0.90647376],\n",
       "        [ 0.9959791 ],\n",
       "        [ 0.19358772],\n",
       "        [-0.98550844]],\n",
       "\n",
       "       [[-0.9999999 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.99999654],\n",
       "        [-0.3447282 ],\n",
       "        [ 0.74606097],\n",
       "        [-0.9973326 ],\n",
       "        [-0.9711084 ],\n",
       "        [-0.7551106 ],\n",
       "        [-0.98904747],\n",
       "        [-0.996454  ],\n",
       "        [-0.9997782 ],\n",
       "        [-0.9999991 ],\n",
       "        [-0.9999998 ],\n",
       "        [-0.9999999 ],\n",
       "        [-0.9999998 ],\n",
       "        [-0.9999971 ],\n",
       "        [-0.9998719 ],\n",
       "        [-0.9941837 ],\n",
       "        [-0.81467414],\n",
       "        [-0.20813428],\n",
       "        [ 0.74818283],\n",
       "        [ 0.9059612 ],\n",
       "        [ 0.7641078 ],\n",
       "        [ 0.5158614 ],\n",
       "        [ 0.4117686 ],\n",
       "        [-0.8781604 ],\n",
       "        [-0.9874219 ]],\n",
       "\n",
       "       [[-0.99999535],\n",
       "        [-0.99999976],\n",
       "        [-1.        ],\n",
       "        [-0.99999785],\n",
       "        [-0.95469666],\n",
       "        [ 0.994314  ],\n",
       "        [-0.31907088],\n",
       "        [-0.6160252 ],\n",
       "        [ 0.27361572],\n",
       "        [-0.9797724 ],\n",
       "        [-0.9995025 ],\n",
       "        [-0.9999849 ],\n",
       "        [-1.        ],\n",
       "        [-0.99999994],\n",
       "        [-0.99999994],\n",
       "        [-0.99999994],\n",
       "        [-0.999999  ],\n",
       "        [-0.99998873],\n",
       "        [-0.9968561 ],\n",
       "        [ 0.34376332],\n",
       "        [ 0.6332113 ],\n",
       "        [ 0.25174615],\n",
       "        [-0.56010354],\n",
       "        [-0.9369684 ],\n",
       "        [-0.9750296 ],\n",
       "        [-0.9990704 ],\n",
       "        [-0.9997781 ],\n",
       "        [-0.9993976 ]],\n",
       "\n",
       "       [[-0.9996249 ],\n",
       "        [-0.9999573 ],\n",
       "        [-0.99999917],\n",
       "        [-0.99999976],\n",
       "        [-0.9999968 ],\n",
       "        [-0.03812132],\n",
       "        [ 0.742142  ],\n",
       "        [ 0.69345963],\n",
       "        [ 0.60298175],\n",
       "        [-0.9481159 ],\n",
       "        [-0.9897033 ],\n",
       "        [-0.9981292 ],\n",
       "        [-0.9999939 ],\n",
       "        [-0.9968484 ],\n",
       "        [-0.9009852 ],\n",
       "        [-0.9154228 ],\n",
       "        [ 0.21597213],\n",
       "        [ 0.64813817],\n",
       "        [ 0.9841541 ],\n",
       "        [ 0.9957799 ],\n",
       "        [-0.77651465],\n",
       "        [-0.9996813 ],\n",
       "        [-0.9999902 ],\n",
       "        [-0.9999969 ],\n",
       "        [-0.99999386],\n",
       "        [-0.9999998 ],\n",
       "        [-0.9999997 ],\n",
       "        [-0.99999535]],\n",
       "\n",
       "       [[-0.9960091 ],\n",
       "        [-0.998348  ],\n",
       "        [-0.99999547],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.99995244],\n",
       "        [-0.6782123 ],\n",
       "        [ 0.950521  ],\n",
       "        [ 0.6105336 ],\n",
       "        [-0.93777734],\n",
       "        [-0.85857296],\n",
       "        [-0.5008565 ],\n",
       "        [-0.97831094],\n",
       "        [ 0.99471986],\n",
       "        [ 0.9997168 ],\n",
       "        [ 0.97197175],\n",
       "        [ 0.9976918 ],\n",
       "        [ 0.98616576],\n",
       "        [ 0.7919428 ],\n",
       "        [-0.94739115],\n",
       "        [-0.9999996 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.99999994]],\n",
       "\n",
       "       [[-0.9970164 ],\n",
       "        [-0.99771327],\n",
       "        [-0.9999963 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.99999976],\n",
       "        [-0.99776417],\n",
       "        [ 0.9371609 ],\n",
       "        [ 0.7240422 ],\n",
       "        [-0.94764227],\n",
       "        [-0.51831794],\n",
       "        [ 0.8163439 ],\n",
       "        [-0.3508085 ],\n",
       "        [ 0.99534625],\n",
       "        [ 0.9020734 ],\n",
       "        [-0.9486233 ],\n",
       "        [-0.05285624],\n",
       "        [-0.9097448 ],\n",
       "        [-0.9981456 ],\n",
       "        [-0.9999894 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ]],\n",
       "\n",
       "       [[-0.99962145],\n",
       "        [-0.99983174],\n",
       "        [-0.99999803],\n",
       "        [-0.99999994],\n",
       "        [-1.        ],\n",
       "        [-0.99999994],\n",
       "        [-0.999929  ],\n",
       "        [ 0.86087656],\n",
       "        [ 0.90263003],\n",
       "        [-0.7923001 ],\n",
       "        [ 0.39126122],\n",
       "        [ 0.9636253 ],\n",
       "        [ 0.17160493],\n",
       "        [ 0.9208812 ],\n",
       "        [ 0.15135995],\n",
       "        [-0.78079444],\n",
       "        [ 0.49142095],\n",
       "        [-0.9369191 ],\n",
       "        [-0.99852866],\n",
       "        [-0.9999909 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ]],\n",
       "\n",
       "       [[-0.99991626],\n",
       "        [-0.9999808 ],\n",
       "        [-0.9999987 ],\n",
       "        [-0.9999998 ],\n",
       "        [-1.        ],\n",
       "        [-0.9999999 ],\n",
       "        [-0.99983954],\n",
       "        [ 0.9487362 ],\n",
       "        [ 0.9909182 ],\n",
       "        [ 0.52796745],\n",
       "        [ 0.92545336],\n",
       "        [ 0.98545176],\n",
       "        [ 0.77127945],\n",
       "        [ 0.9379731 ],\n",
       "        [ 0.9016286 ],\n",
       "        [ 0.90067136],\n",
       "        [ 0.8327178 ],\n",
       "        [-0.9842868 ],\n",
       "        [-0.99971044],\n",
       "        [-0.9999996 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ]],\n",
       "\n",
       "       [[-0.99990094],\n",
       "        [-0.9999708 ],\n",
       "        [-0.99999267],\n",
       "        [-0.9999866 ],\n",
       "        [-0.9999976 ],\n",
       "        [-0.99997455],\n",
       "        [-0.9299593 ],\n",
       "        [ 0.9968224 ],\n",
       "        [ 0.9848509 ],\n",
       "        [ 0.4209187 ],\n",
       "        [ 0.73994124],\n",
       "        [ 0.9300956 ],\n",
       "        [ 0.90208966],\n",
       "        [ 0.9605801 ],\n",
       "        [ 0.89871365],\n",
       "        [ 0.72415054],\n",
       "        [-0.8243794 ],\n",
       "        [-0.9996305 ],\n",
       "        [-0.99999404],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ]],\n",
       "\n",
       "       [[-0.9999773 ],\n",
       "        [-0.9999921 ],\n",
       "        [-0.999996  ],\n",
       "        [-0.9998742 ],\n",
       "        [-0.99944484],\n",
       "        [-0.9816662 ],\n",
       "        [ 0.7043569 ],\n",
       "        [ 0.9713344 ],\n",
       "        [-0.16974825],\n",
       "        [-0.8696637 ],\n",
       "        [-0.9129123 ],\n",
       "        [-0.79156697],\n",
       "        [-0.39271465],\n",
       "        [ 0.47986543],\n",
       "        [ 0.08552492],\n",
       "        [-0.881802  ],\n",
       "        [-0.99854606],\n",
       "        [-0.99998736],\n",
       "        [-0.99999976],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ]],\n",
       "\n",
       "       [[-0.9999989 ],\n",
       "        [-0.9999998 ],\n",
       "        [-0.9999999 ],\n",
       "        [-0.9999013 ],\n",
       "        [-0.9914143 ],\n",
       "        [-0.6305686 ],\n",
       "        [ 0.30217695],\n",
       "        [-0.86912125],\n",
       "        [-0.9954278 ],\n",
       "        [-0.99940044],\n",
       "        [-0.99996275],\n",
       "        [-0.9999916 ],\n",
       "        [-0.9999852 ],\n",
       "        [-0.9998968 ],\n",
       "        [-0.9998448 ],\n",
       "        [-0.9999899 ],\n",
       "        [-0.9999995 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ]],\n",
       "\n",
       "       [[-0.99999994],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.99999017],\n",
       "        [-0.99433786],\n",
       "        [-0.8356134 ],\n",
       "        [-0.9616629 ],\n",
       "        [-0.99880946],\n",
       "        [-0.9993915 ],\n",
       "        [-0.99998593],\n",
       "        [-0.9999999 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ]],\n",
       "\n",
       "       [[-0.99999994],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.9999967 ],\n",
       "        [-0.99899924],\n",
       "        [-0.9941209 ],\n",
       "        [-0.9992996 ],\n",
       "        [-0.99976826],\n",
       "        [-0.999379  ],\n",
       "        [-0.99998015],\n",
       "        [-0.9999998 ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ]],\n",
       "\n",
       "       [[-0.99999905],\n",
       "        [-0.9999999 ],\n",
       "        [-0.99999994],\n",
       "        [-0.99998593],\n",
       "        [-0.99974173],\n",
       "        [-0.9997256 ],\n",
       "        [-0.9999405 ],\n",
       "        [-0.9998241 ],\n",
       "        [-0.9974189 ],\n",
       "        [-0.99906874],\n",
       "        [-0.99997497],\n",
       "        [-0.99999946],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.99999994],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-1.        ],\n",
       "        [-0.9999999 ]],\n",
       "\n",
       "       [[-0.9999111 ],\n",
       "        [-0.9999919 ],\n",
       "        [-0.99999726],\n",
       "        [-0.99992234],\n",
       "        [-0.99989325],\n",
       "        [-0.9999128 ],\n",
       "        [-0.999931  ],\n",
       "        [-0.99961156],\n",
       "        [-0.99376714],\n",
       "        [-0.9889119 ],\n",
       "        [-0.9991269 ],\n",
       "        [-0.99995345],\n",
       "        [-0.999995  ],\n",
       "        [-0.9999942 ],\n",
       "        [-0.99994814],\n",
       "        [-0.99994797],\n",
       "        [-0.9999675 ],\n",
       "        [-0.9999874 ],\n",
       "        [-0.9999967 ],\n",
       "        [-0.9999993 ],\n",
       "        [-0.99999994],\n",
       "        [-1.        ],\n",
       "        [-0.99999994],\n",
       "        [-0.99999905],\n",
       "        [-0.999992  ],\n",
       "        [-0.99999624],\n",
       "        [-0.99999833],\n",
       "        [-0.99997413]],\n",
       "\n",
       "       [[-0.9958938 ],\n",
       "        [-0.9992326 ],\n",
       "        [-0.99952936],\n",
       "        [-0.9986091 ],\n",
       "        [-0.999345  ],\n",
       "        [-0.9994035 ],\n",
       "        [-0.99930286],\n",
       "        [-0.99790764],\n",
       "        [-0.9824024 ],\n",
       "        [-0.97554016],\n",
       "        [-0.9945545 ],\n",
       "        [-0.99857914],\n",
       "        [-0.9992997 ],\n",
       "        [-0.9982991 ],\n",
       "        [-0.989206  ],\n",
       "        [-0.9859451 ],\n",
       "        [-0.98850024],\n",
       "        [-0.99376905],\n",
       "        [-0.995735  ],\n",
       "        [-0.99659175],\n",
       "        [-0.99888533],\n",
       "        [-0.999405  ],\n",
       "        [-0.9989149 ],\n",
       "        [-0.99735266],\n",
       "        [-0.99668545],\n",
       "        [-0.9994354 ],\n",
       "        [-0.9998666 ],\n",
       "        [-0.9985533 ]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_imgs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
