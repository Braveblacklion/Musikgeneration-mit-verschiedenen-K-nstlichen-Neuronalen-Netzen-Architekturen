{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains functions for loading the metric logs and printing them in nice format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "import warnings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metric(filepath, start_step=0):\n",
    "    \"\"\"Load a metric file and return a dictionary containing metric arrays.\"\"\"\n",
    "    data = np.load(filepath, allow_pickle = True) # pickle\n",
    "    return (data[()]['score_matrix_mean'], data[()]['score_pair_matrix_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_names = ('Drums', 'Piano', 'Guitar', 'Bass', 'Ensemble', 'Reed',\n",
    "               'Synth Lead', 'Synth Pad')\n",
    "metric_names = ('empty bar rate', '# of pitch used', 'qualified note rate',\n",
    "                'polyphonicity', 'note in scale', 'drum in pattern rate',\n",
    "                '# of chroma used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "traing_data_eval_path = './data/eval_training_data/lastfm_alternative_8b_phrase.npy' # 'eval_' dazu\n",
    "data_dir = './data/eval_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    #('pretrained (01)', \"test_lastfm_alternative_pretrain_g_proposed_d_proposed_b.npy\"),\n",
    "    ##('pretrained (01r)', \"test_lastfm_alternative_pretrain_g_proposed_d_proposed_r.npy\"),\n",
    "    ('pretrained (02)', \"05_Notfall_140_pretrain_g_proposed_d_proposed_b.npy\"),\n",
    "    #('pretrained (02r)', \"05_Notfall_140_pretrain_g_proposed_d_proposed_r.npy\"),\n",
    "    #('pretrained (03)', \"03_Optimal_150_pretrain_g_proposed_d_proposed.npy\"),\n",
    "    #('pretrained (04)', \"04_sub_Optimal_84_200_pretrain_g_proposed_d_proposed.npy\"),\n",
    "    #('train (01b)', \"test_lastfm_alternative_train_g_proposed_d_proposed_r_proposed_bernoulli.npy\"),\n",
    "    ('train (02b)', \"05_Notfall_140_train_g_proposed_d_proposed_r_proposed_bernoulli.npy\"),\n",
    "    ('train (02r)', \"05_Notfall_140_train_g_proposed_d_proposed_r_proposed_round.npy\"),\n",
    "    ('train (03r)', \"03_Optimal_150_train_g_proposed_d_proposed_r_proposed_round.npy\"),\n",
    "    #('train (04r)', \"04_sub_Optimal_84_200_train_g_proposed_d_proposed_r_proposed_round.npy\"),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = [('training data', load_metric(traing_data_eval_path))]\n",
    "for filename in filenames:\n",
    "    score_list.append((filename[0],\n",
    "                       load_metric(os.path.join(data_dir, filename[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metric_table(m):\n",
    "    print('='*30 + \"\\n{:=^30}\\n\".format(' ' + metric_names[m] + ' ') + '='*30)\n",
    "    for entry in score_list:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            mean = np.nanmean(entry[1][0][m, :])\n",
    "        print(\"{:24} {:5.2f}\".format(entry[0], mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intratrack Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "======= empty bar rate =======\n",
      "==============================\n",
      "training data             0.57\n",
      "pretrained (02)           0.51\n",
      "train (02b)               0.56\n",
      "train (02r)               0.63\n",
      "train (03r)               0.48\n",
      "==============================\n",
      "====== # of pitch used =======\n",
      "==============================\n",
      "training data             4.66\n",
      "pretrained (02)           4.49\n",
      "train (02b)               4.95\n",
      "train (02r)               6.88\n",
      "train (03r)               4.84\n",
      "==============================\n",
      "==== qualified note rate =====\n",
      "==============================\n",
      "training data             0.88\n",
      "pretrained (02)           0.43\n",
      "train (02b)               0.86\n",
      "train (02r)               0.84\n",
      "train (03r)               0.83\n",
      "==============================\n",
      "======= polyphonicity ========\n",
      "==============================\n",
      "training data             0.45\n",
      "pretrained (02)           0.24\n",
      "train (02b)               0.31\n",
      "train (02r)               0.50\n",
      "train (03r)               0.26\n",
      "==============================\n",
      "======= note in scale ========\n",
      "==============================\n",
      "training data             0.59\n",
      "pretrained (02)           0.63\n",
      "train (02b)               0.69\n",
      "train (02r)               0.70\n",
      "train (03r)               0.69\n",
      "==============================\n",
      "==== drum in pattern rate ====\n",
      "==============================\n",
      "training data             0.92\n",
      "pretrained (02)           0.53\n",
      "train (02b)               0.52\n",
      "train (02r)               0.44\n",
      "train (03r)               0.54\n",
      "==============================\n",
      "====== # of chroma used ======\n",
      "==============================\n",
      "training data             2.60\n",
      "pretrained (02)           2.46\n",
      "train (02b)               2.85\n",
      "train (02r)               3.31\n",
      "train (03r)               2.46\n"
     ]
    }
   ],
   "source": [
    "for m in range(7):\n",
    "    print_metric_table(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intertrack Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "======= tonal distance =======\n",
      "==============================\n",
      "training data             0.96\n",
      "pretrained (02)           1.13\n",
      "train (02b)               1.12\n",
      "train (02r)               1.26\n",
      "train (03r)               0.92\n"
     ]
    }
   ],
   "source": [
    "print('='*30 + \"\\n{:=^30}\\n\".format(' tonal distance ') + '='*30)\n",
    "for entry in score_list:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        mean = np.nanmean(entry[1][1])\n",
    "    print(\"{:24} {:5.2f}\".format(entry[0], mean))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
